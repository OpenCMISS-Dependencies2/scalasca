/****************************************************************************
**  SCALASCA    http://www.scalasca.org/                                   **
*****************************************************************************
**  Copyright (c) 1998-2019                                                **
**  Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre          **
**                                                                         **
**  Copyright (c) 2009-2013                                                **
**  German Research School for Simulation Sciences GmbH,                   **
**  Laboratory for Parallel Programming                                    **
**                                                                         **
**  This software may be modified and distributed under the terms of       **
**  a BSD-style license.  See the COPYING file in the package base         **
**  directory for details.                                                 **
****************************************************************************/


/**
@page start Getting started

This chapter provides an introduction to the use of the Scalasca Trace Tools on the basis of the analysis of an example application.
The most prominent features are addressed, and at times a reference to later chapters with more in-depth information on the corresponding topic is given.

Use of the Scalasca Trace Tools involves three phases: instrumentation of the target application, execution measurement collection and analysis, and examination of the analysis report.
For instrumentation and measurement, the Scalasca Trace Tools 2.x release series leverages the Score-P infrastructure, while the Cube graphical user interface is used for analysis report examination.
The Scalasca Trace Tools complement the functionality provided by Score-P and Cube with scalable automatic trace-analysis components, as well as convenience commands for controlling execution measurement collection and analysis, and analysis report post-processing.

Most of Scalasca's functionality can be accessed through the `scalasca` command, which provides action options that in turn invoke the corresponding underlying commands `scorep`, `scan` and `square`.
These actions are:

 1. `scalasca -instrument`

    (or short `skin`) familiar to users of the Scalasca 1.x series is <b>deprecated</b> and only provided for backward compatibility.
    It tries to map the command-line options of the Scalasca 1.x instrumenter onto corresponding options of Score-P's instrumenter command `scorep`---as far as this is possible.
    However, to take full advantage of the functionality provided by Score-P, <b>users are strongly encouraged to use the `scorep` instrumenter command directly</b>.
    To assist in transitioning existing measurement configurations to Score-P, the Scalasca instrumentation wrapper prints the converted command that is actually executed to standard output.

 2. `scalasca -analyze`

    (or short `scan`) is used to control the Score-P measurement environment during the execution of the target application---supporting both runtime summarization and/or event trace collection, optionally including hardware-counter information---and to automatically initiate Scalasca's trace analysis after measurement completion if tracing was requested.

 3. `scalasca -examine`

    (or short `square`) is used to post-process the analysis report generated by a Score-P profiling measurement and/or Scalasca's automatic post-mortem trace analysis, and to start the Cube graphical user interface for analysis report examination.

To get a brief usage summary, call the `scalasca` command without arguments, or use `scalasca --quickref` to open the Scalasca Quick Reference (with a suitable PDF viewer).
See also Section @ref scalasca for a complete reference of the `scalasca` command.

@note
    Under the hood, the Scalasca convenience commands leverage a number of other commands provided by Score-P as well as the CubeLib and CubeGUI components.
    Therefore, it is generally advisable to include the executable directories of appropriate installations of all those components in the shell search path (`$PATH`).

The following three sections provide a quick overview of each of these actions and how to use them during the corresponding step of the performance analysis, before a tutorial-style full workflow example is presented in Section @ref start_workflow.

@seclist
    @secitem{start_instrumentation}
    @secitem{start_measurement}
    @secitem{start_examination}
    @secitem{start_workflow}
@endseclist

@navigation_both{intro,commands}
*/


//---------------------------------------------------------------------------

/**
@page start_instrumentation Instrumentation

To generate measurements which can be used as input for the Scalasca Trace Tools, user application programs first need to be instrumented.
That is, special measurement calls have to be inserted into the program code which are then executed at specific important points (events) during the application run.
Unlike previous versions of Scalasca which used a custom measurement system, this task is now accomplished by the community instrumentation and measurement infrastructure Score-P.

As already mentioned in the previous section, use of the `scalasca -instrument` and `skin` commands is discouraged, and therefore not discussed in this guide.
Instead, all the necessary instrumentation of user routines, OpenMP constructs, MPI functions, etc. should be handled by the Score-P instrumenter, which is accessed through the `scorep` command.
Therefore, the compile and link commands to build the application that is to be analyzed should be prefixed with `scorep` (e.g., in a Makefile).

For example, to instrument the MPI application executable `myapp` generated from the two Fortran source files `foo.f90` and `bar.f90`, the following compile and link commands

@verbatim
  $ mpif90 -c foo.f90
  $ mpif90 -c bar.f90
  $ mpif90 -o myapp foo.o bar.o
@endverbatim

have to be replaced by corresponding commands involving the Score-P instrumenter:

@verbatim
  $ scorep  mpif90 -c foo.f90
  $ scorep  mpif90 -c bar.f90
  $ scorep  mpif90 -o myapp foo.o bar.o
@endverbatim

This will automatically instrument every routine entry and exit seen by the compiler, intercept MPI function calls to gather message-passing information, and link the necessary Score-P measurement libraries to the application executable.

Alternatively, the Score-P compiler wrapper commands (e.g., `scorep-mpif90`) can be used as a <em>compiler replacement</em>.
These commands allow to control instrumentation via an environment variable, which may be required during the configuration step with certain build systems such as CMake or GNU Autotools.
Please refer to the Section &quot;Score-P Compiler Wrapper Usage&quot; in the Score-P User Manual @cite scorep_manual for details.

@attention
    <b>The Score-P instrumenter commands (`scorep` prefix or compiler wrapper) must be used with the link command</b> to ensure that all required Score-P measurement libraries are linked with the executable.
    However, not all object files need to be instrumented, thereby avoiding measurements and data collection for routines and OpenMP constructs defined in those files.
    Nevertheless, for the Scalasca trace analysis to work correctly, <b>instrumenting files defining OpenMP parallel regions is essential</b>.

Although generally most convenient, automatic compiler-based function instrumentation as used by default may result in too many and/or too disruptive measurements, which can be addressed by selective instrumentation and measurement filtering.
While the most basic steps will be briefly covered in Section @ref start_workflow_filtering, please also consult the Score-P manual @cite scorep_manual for details on the available instrumentation and filtering options.

@navigation_next{start_measurement}
*/

//---------------------------------------------------------------------------

/**
@page start_measurement Runtime measurement collection & analysis

While applications instrumented by Score-P can be executed directly with a measurement configuration defined via environment variables, the `scalasca -analyze` (or short `scan`) convenience command provided by Scalasca can be used to control certain aspects of the Score-P measurement environment during the execution of the target application.
To produce a performance measurement using an instrumented executable, the target application execution command is prefixed with the `scalasca -analyze` (or short `scan`) command:

@verbatim
  $ scalasca -analyze [options] \
             [<launch_cmd> [<launch_flags>]] <target> [<target_args>]
@endverbatim

For pure MPI or hybrid MPI+OpenMP/Pthreads applications, @a launch_cmd is typically the MPI execution command such as `mpirun` or `mpiexec`, with @a launch_flags being the corresponding command-line arguments as used for uninstrumented runs, for example, to specify the number of compute nodes or MPI ranks.
For non-MPI (i.e., serial and pure multi-threaded) applications, the launch command and flags can usually be omitted.

In case of the example MPI application executable `myapp` introduced in the previous section, a measurement command starting the application with four MPI ranks could therefore be:

@verbatim
  $ scalasca -analyze  mpiexec -n 4 ./myapp
@endverbatim

@attention
    A unique directory is used for each measurement experiment, which (by default) must not already exist when measurement starts: otherwise measurement is aborted immediately.

A default name for the experiment directory is composed of the prefix &quot;`scorep_`&quot;, the target application executable name, the run configuration (e.g., number of MPI ranks and/or OpenMP threads), and a few other parameters of the measurement configuration.
For example, a measurement of the `myapp` application as outlined above will produce a measurement experiment directory named &quot;`scorep_myapp_4_sum`&quot;.

@note
    A number of settings regarding the measurement configuration can be specified in different ways.
    See the `scan` command reference in Section @ref scan for details and available configuration options.

When measurement has completed, the measurement experiment directory contains various log files and one or more analysis reports.
By default, runtime summarization is used to provide a summary report of the number of visits and time spent on each callpath by each process/thread, as well as hardware counter metrics (if configured).
For MPI or hybrid MPI+OpenMP/Pthreads applications, MPI message statistics are also included.

Event trace data can also be collected as part of a measurement.
This measurement mode can be enabled by passing the `-t` option to the `scalasca -analyze` command (or alternatively by setting the environment variable `SCOREP_ENABLE_TRACING` to either &quot;`1`&quot;, &quot;`true`&quot;, or &quot;`yes`&quot;).

@note
    Enabling event trace collection does not automatically turn off summarization mode (i.e., both a summary profile and event traces are collected).
    It has to be explicitly disabled when this behavior is undesired.

When collecting a trace measurement, experiment trace analysis is automatically initiated after measurement is complete to quantify wait states that cannot be detected with runtime summarization, to determine their root causes, and to identify the critical path of the application.
In addition to examining the trace-analysis report, the generated event traces can also be visualized with a third-party graphical trace browser such as Vampir @cite knuepfer_ea:2008.

@warning
    <b>Traces can easily become extremely large and unwieldy</b>, and uncoordinated intermediate trace buffer flushes may result in cascades of distortion, which renders such traces to be of little value.
    <b>It is therefore extremely important to set up an adequate measurement configuration <i>before</i> initiating trace collection and analysis!</b>
    Please see Section @ref start_workflow_filtering as well as the Score-P User Manual @cite scorep_manual for more details on how to set up a filtering file and adjust Score-P's internal memory management.

@navigation_both{start_instrumentation,start_examination}
*/


//---------------------------------------------------------------------------

/**
@page start_examination Analysis report examination

The results of the runtime summarization and/or the automatic trace analysis are stored in one or more reports (i.e., CUBE4 files) in the measurement experiment directory.
These reports can be post-processed and examined using the `scalasca -examine` (or short `square`) command, providing an experiment directory name as argument:

@verbatim
  $ scalasca -examine [options] <experiment_name>
@endverbatim

Post-processing is performed leveraging commands provided by the CubeLib component the first time an experiment is examined, before launching the Cube analysis report browser (CubeGUI).
If the `scalasca -examine` command is provided with an already processed experiment directory, or with a CUBE4 file specified as argument, the viewer is launched immediately.

Instead of interactively examining the measurement analysis results, a textual score report can also be obtained using the `-s` option (see Section @ref square for further command-line options) without launching the viewer:

@verbatim
  $ scalasca -examine -s <experiment_name>
@endverbatim

This score report is generated by Score-P's `scorep-score` utility and provides a breakdown of the different types of regions included in the measurement and their estimated associated trace buffer capacity requirements, aggregate trace size, and largest process trace buffer size, which can be used to set up a filtering file and to determine an appropriate setting for `SCOREP_TOTAL_MEMORY` to be used for subsequent trace measurements.
See Section @ref start_workflow_filtering for more details.

The Cube viewer can also be directly used on an experiment archive---opening a dialog window to choose one of the contained CUBE4 files---or an individual CUBE4 file as shown below:

@verbatim
  $ cube <experiment_name>
  $ cube <file>.cubex
@endverbatim

However, keep in mind that no post-processing is performed in this case, so that only a subset of Scalasca's analyses and metrics may be shown.

@navigation_both{start_measurement,start_workflow}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow A full workflow example

While the previous sections introduced the three basic actions supported by the Scalasca Trace Tools based on an abstract example, this section will now guide through the typical analysis workflow using a moderately complex, MPI-based benchmark code: BT from the NAS Parallel Benchmarks (NPB-MPI 3.3.1) @cite npb_website.
The BT benchmark implements a simulated computational fluid dynamics (CFD) application using a block-tridiagonal solver for a synthetic system of nonlinear partial differential equations and consists of about 20 Fortran 77 source code files.
Although BT does not exhibit significant performance bottlenecks---after all, it is a highly optimized benchmark---it serves as a good example to demonstrate the overall workflow, including typical configuration steps and how to avoid common pitfalls.

The example measurements shown in this section were carried out using the Scalasca Trace Tools v@scalasca_version in conjunction with Score-P v@scorep_version, CubeLib v@cubelib_version, and CubeGUI v@cubegui_version on the JURECA cluster at J&uuml;lich Supercomputing Centre.
JURECA's compute nodes are equipped with two Intel Xeon E5-2680 v3 (Haswell) 12-core CPUs running at 2.5 GHz, and connected via an EDR InfiniBand fat-tree network.
The BT benchmark code was compiled using Intel compilers and linked against ParTec ParaStation MPI (which is based on MPICH).
The example commands shown below should therefore be representative for using the Scalasca Trace Tools in a typical HPC cluster environment.
For convenience, the resulting post-processed Cube files are also available for download on the Scalasca documentation web page @cite scalasca_webdocs.

@note
    In the following, it is assumed that all Scalasca commands are available in the shell's search path (`$PATH`), for example, after loading site-specific environment modules.
    Also, remember that the Scalasca convenience commands use other executables provided by Score-P, CubeLib, and CubeGUI, which therefore need to be available in the search path as well.

@seclist
    @secitem{start_workflow_reference}
    @secitem{start_workflow_instrument}
    @secitem{start_workflow_initial_summary}
    @secitem{start_workflow_filtering}
    @secitem{start_workflow_summary}
    @secitem{start_workflow_trace}
@endseclist

@navigation_prev{start_examination}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_reference Preparing a reference execution

As a first step of every performance analysis, a reference execution using an uninstrumented executable should be performed.
First, this step verifies that the code executes cleanly and produces correct results.
Second, it later allows to assess the run-time overhead introduced by instrumentation and measurement.
And finally, it provides a baseline to compare with after applying some code optimizations.
At this stage an appropriate test configuration should be chosen, such that it is both repeatable and long enough to be representative.
(Note that excessively long execution durations can make measurement analysis inconvenient or even prohibitive, and therefore should be avoided.)

After unpacking the NPB-MPI source archive, the build system has to be adjusted to the respective environment.
For the NAS benchmarks, this is accomplished by a Makefile snippet defining a number of variables used by a generic Makefile.
This snippet is called `make.def` and has to reside in the `config/` subdirectory, which already contains a template file that can be copied and adjusted appropriately.
In particular, the MPI Fortran compiler wrapper and flags need to be specified, for example:

@verbinclude start_make.def

Note that the MPI C compiler wrapper and flags are not used for building BT, but may also be set in the `config/make.def` file accordingly to experiment with other NPB benchmarks.

Next, the benchmark can be built from the top-level directory by running `make`, specifying the number of MPI ranks to use via the `NPROCS` variable---for BT, this is required to be a square number---as well as the problem size via the `CLASS` variable on the command line.
Valid problem classes (of increasing size) are W, S, A, B, C, D, and E, and can be used to adjust the benchmark runtime to the execution environment.
For example, class W or S is appropriate for execution on a laptop with 4 MPI ranks, while the other problem sizes are more suitable for &quot;real&quot; configurations.
For the example run on JURECA, 144 MPI ranks and problem class D have been chosen:

@verbinclude start_make.log

The resulting executable encodes the benchmark configuration in its name and is placed into the `bin/` subdirectory.
For the example `make` command above, it is named `bt.D.144`.
This binary can now be executed, either via submitting an appropriate batch job (which is beyond the scope of this user guide) or directly in an interactive session.

@verbinclude start_reference.log

In the selected configuration, the BT benchmark executes 250 iterations of the time step loop, and then verifies that the result matches the expected outcome.
Before exiting, the benchmark also reports some configuration details, as well as the wall-clock execution time (216.00 seconds) for the core computation.

@navigation_next{start_workflow_instrument}
*/

//---------------------------------------------------------------------------

/**
@page start_workflow_instrument Instrumenting the application code

Now that the reference execution was successful, it is time to prepare an instrumented executable using Score-P to perform an initial measurement.
By default, Score-P leverages the compiler to automatically instrument every function entry and exit.
This is usually the best first approach if one does not have detailed knowledge about the application and needs to identify the hotspots in the code.
For BT, using Score-P for instrumentation is simply accomplished by prefixing the compile and link commands specified in the `config/make.def` Makefile snippet by the Score-P instrumenter command `scorep`:

@verbinclude start_instrument.def

Note that the linker specification variable `FLINK` in `config/make.def` defaults to the value of `MPIF77`, that is, no further modifications are necessary in this case.

Recompilation of the BT source code in the top-level directory now creates an instrumented executable, overwriting the uninstrumented binary (for archiving purposes, one may consider renaming it before recompiling):

@verbinclude start_instrument.log

@navigation_both{start_workflow_reference,start_workflow_initial_summary}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_initial_summary Initial summary measurement

The instrumented executable prepared in the previous step can now be executed under the control of the `scalasca -analyze` (or short `scan`) convenience command to perform an initial summary measurement:

@verbinclude start_initial.log

As can be seen, the measurement run successfully produced an experiment directory named `scorep_bt_144_sum` containing
 - a text file `MANIFEST.md` briefly describing the directory contents produced by the Score-P measurement system,
 - the runtime summary result file `profile.cubex`,
 - a copy of the measurement configuration in `scorep.cfg`, and
 - the measurement log file `scorep.log`.

However, application execution took almost twice as long as the reference run (413.25 vs. 216.00 seconds).
That is, instrumentation and associated measurements introduced a non-negligible amount of run-time overhead.
While it is possible to interactively examine the generated summary result file using the Cube report browser, this should only be done with great caution since the substantial overhead negatively impacts the accuracy of the measurement.
Therefore, such measurements can easily be misleading.

@navigation_both{start_workflow_instrument,start_workflow_filtering}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_filtering Optimizing the measurement configuration

To avoid drawing wrong conclusions based on skewed performance data due to excessive measurement overhead, it is often necessary to optimize the measurement configuration before conducting any additional experiments.
This can be achieved in various ways, for example, using runtime filtering, selective recording, or manual instrumentation controlling measurement.
Please consult the Score-P Manual @cite scorep_manual for details on the available options.
However, in many cases it is already sufficient to filter a small number of frequently executed but computationally inexpensive user functions to reduce the measurement overhead to an acceptable level.
In this context, filtering means that those functions are still executed, but no measurements are taken and recorded for them.
Therefore, filtered functions no longer show up in the measurement report, and the associated execution time is attributed to the parent function from which they are called (similar to inlining performed by the compiler).
The selection of the routines to be filtered has to be done with care, though, as it affects the granularity of the measurement and too aggressive filtering might &quot;blur&quot; the location of important hotspots.

To assist in identifying candidate functions for runtime filtering, the initial summary report can be scored using the `-s` option of the `scalasca -examine` command:

@verbinclude start_score.log

As can be seen from the top of the score report, the estimated size for an event trace measurement without filtering applied is approximately 3.7 TiB, with the process-local maximum across all ranks being roughly 26 GiB.
Considering the 128 GiB of main memory available on JURECA's compute nodes, the 24 MPI ranks per node, and the fact that Score-P's internal memory buffer is limited to 4 GiB per process, a tracing experiment with this configuration is clearly prohibitive if disruptive intermediate trace buffer flushes are to be avoided.

The next section of the score output provides a table which shows how the trace memory requirements of a single process (column `max_buf`) as well as the overall number of visits and CPU allocation time are distributed among certain function groups.
For traces that can be handled by the Scalasca Trace Tools, the most relevant groups are:
 - <b>`MPI`</b>:
   MPI API functions.
 - <b>`OMP`</b>:
   OpenMP constructs and API functions.
 - <b>`PTHREAD`</b>:
   POSIX threads API functions.
 - <b>`COM`</b>:
   User functions/regions that appear on a call path to a parallelization API call or construct (MPI/OpenMP/POSIX threads).
   These functions provide the context of parallelization API usage and should therefore only be filtered with care.
 - <b>`USR`</b>:
   User functions/regions that do not appear on a call path to a parallelization API call or construct (MPI/OpenMP/POSIX threads).
 - <b>`SCOREP`</b>:
   Artificial regions generated by the Score-P measurement system.

The detailed breakdown by region below the summary provides a classification according to these function groups (column `type`) for each region found in the summary report.
Investigation of this part of the score report reveals that most of the trace data would be generated by about 50 billion calls to each of the three routines `matvec_sub`, `binvcrhs` and `matmul_sub`, which are classified as `USR`.
And although the percentage of time spent in these routines at first glance suggest that they are important, the average time per visit is below 170 nanoseconds (column `time/visit`).
That is, the relative measurement overhead for these functions is substantial, and thus a significant amount of the reported time is very likely spent in the Score-P measurement system rather than in the application itself.
Therefore, these routines constitute good candidates for being filtered (like they are good candidates for being inlined by the compiler).
Additionally selecting the `exact_solution` routine, which generates about 200 MiB of event data on a single rank with very little runtime impact, a reasonable Score-P filtering file would therefore look like this:

@verbinclude npb-bt.filt

Please refer to the Score-P User Manual @cite scorep_manual for a detailed description of the filter file format, how to filter based on file names, define (and combine) blacklists and whitelists, and how to use wildcards for convenience.
Also note that the run-time filtering approach used in this example only affects routines in the `USR` and `COM` groups.
Measurements for other groups can---to certain degrees---be controlled by other means, as the generated events have to meet various consistency requirements.

The effectiveness of this filter---in terms of generated trace data---can be examined by scoring the initial summary report again, this time also specifying the filter file using the `-f` option of the `scalasca -examine` command.
This way a filter file can be incrementally developed, avoiding the need to conduct many measurements to step-by-step investigate the effect of filtering individual functions.

@verbinclude start_score_filt.log

Below the (original) function group summary, the score report now also includes a second summary with the filter applied.
Here, an additional group `FLT` is added, which subsumes all filtered regions.
Moreover, the column `flt` indicates whether a region/function group is filtered (&quot;`+`&quot;), not filtered (&quot;`-`&quot;), or possibly partially filtered (&quot;`*`&quot;, only used for function groups).

As expected, the estimate for the aggregate event trace size drops down to 3.9 GiB, and the process-local maximum across all ranks is reduced to 28 MiB.
Since the Score-P measurement system also creates a number of internal data structures (e.g., to track MPI requests and communicators), the suggested setting for the `SCOREP_TOTAL_MEMORY` environment variable to adjust the maximum amount of memory used by the Score-P memory management is 30 MiB when tracing is configured (see Section @ref start_workflow_trace).

@navigation_both{start_workflow_initial_summary,start_workflow_summary}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_summary Summary measurement & examination

The filtering file prepared in Section @ref start_workflow_filtering can now be applied to produce a new summary measurement, ideally with reduced measurement overhead to improve accuracy.
This can be accomplished by providing the filter file name to `scalasca -analyze` via the `-f` option.

@attention
    Before re-analyzing the application, the unfiltered summary experiment should be renamed (or removed), since `scalasca -analyze` will by default not overwrite the existing experiment directory and abort immediately.

@verbinclude start_summary.log

This new measurement produced an experiment directory containing one additional file compared to the initial run: a copy of the measurement filter in `scorep.filter`.
Notice that applying the runtime filtering reduced the measurement overhead significantly, down to now only 5.5% (228.02 seconds vs. 216.00 seconds for the reference run).
This new measurement with the optimized configuration should therefore quite accurately represent the real runtime behavior of the BT application, and can now be post-processed and interactively explored using the Cube result browser.
These two steps can be conveniently initiated using the `scalasca -examine` command:

@verbinclude start_summary_exam.log

Examination of the summary result (see Figure @ref SummaryExperiment for a screenshot and Section @ref start_workflow_cube for a brief summary of how to use the Cube browser) shows that 96.5% of the overall CPU allocation time is spent in computations, while 3% of the time is spent in MPI point-to-point communication functions and the remainder scattered across other activities.
The point-to-point time is almost entirely spent in `MPI_Wait` calls inside the three solver functions `x_solve`, `y_solve` and `z_solve`, as well as an `MPI_Waitall` in the boundary exchange routine `copy_faces`.
Computation time is also mostly spent in the solver routines and the boundary exchange, however, inside the different `solve_cell`, `backsubstitute` and `compute_rhs` functions.
While the aggregated time spent in the computational routines seems to be relatively balanced across the different MPI ranks (determined using the box plot view in the right pane), there is quite some variation for the `MPI_Wait` / `MPI_Waitall` calls.

@figure{start_summary.png,SummaryExperiment,Screenshot of a summary experiment result in the Cube report browser.,width=\textwidth}


@section start_workflow_cube Using the Cube browser

The following paragraphs provide a very brief introduction to the usage of the Cube analysis report browser.
To make effective use of the GUI, however, please also consult the CubeGUI User Guide @cite cube_manual.

Cube is a generic user interface for presenting and browsing performance and debugging information from parallel applications.
The underlying data model is independent from particular performance properties to be displayed.
The Cube main window (see Figure @ref SummaryExperiment) consists of three panels containing tree displays or alternate graphical views of analysis reports.
The left panel shows performance properties of the execution, such as time or the number of visits.
The middle pane shows the call tree or a flat profile of the application.
The right pane either shows the system hierarchy consisting of, for example, machines, compute nodes, processes, and threads, or various graphical representations such as a topological view of the application's processes and threads (if available), or a box/violin plot view showing the statistical distribution of values across the system.
All tree nodes are labeled with a metric value and a color-coded box which can help in identifying hotspots.
The metric value color is determined from the proportion of the total (root) value or some other specified reference value, using the color scale at the bottom of the window.

A click on a performance property or a call path selects the corresponding node.
This has the effect that the metric value held by this node (such as execution time) will be further broken down into its constituents in the panels right of the selected node.
For example, after selecting a performance property, the middle panel shows its distribution across the call tree.
After selecting a call path (i.e., a node in the call tree), the system tree shows the distribution of the performance property in that call path across the system locations.
A click on the icon to the left of a node in each tree expands or collapses that node.
By expanding or collapsing nodes in each of the three trees, the analysis results can be viewed on different levels of granularity (inclusive vs. exclusive values).

All tree displays support a context menu, which is accessible using the right mouse button and provides further options.
For example, to obtain the exact definition of a performance property, select &quot;Documentation&quot; in the context menu associated with each performance property.
A brief description can also be obtained from the menu option &quot;Info&quot;.

@navigation_both{start_workflow_filtering,start_workflow_trace}
*/


//---------------------------------------------------------------------------

/**
@page start_workflow_trace Trace collection and analysis

While summary profiles only provide process- or thread-local data aggregated over time, event traces contain detailed time-stamped event data which also allows to reconstruct the dynamic behavior of an application.
This enables tools such as the Scalasca trace analyzer to provide even more insights into the performance behavior of an application, for example, whether the time spent in MPI communication is real message processing time or incurs significant wait states (i.e., intervals where a process sits idle without doing useful work waiting for data from other processes to arrive).

Trace collection and subsequent automatic analysis by the Scalasca trace analyzer can be enabled using the `-t` option of `scalasca -analyze`.
Since this option enables trace collection <i>in addition</i> to collecting a summary measurement, it is often used in conjunction with the `-q` option which turns off measurement entirely.
(Note that the order in which these two options are specified matters: First turn off measurement using `-q`, then enable tracing with `-t`.)

@attention
    <b>Do not forget to specify an appropriate measurement configuration</b> (i.e., a filtering file and `SCOREP_TOTAL_MEMORY` setting)!
    Otherwise, you may easily fill up your disks and suffer from uncoordinated intermediate trace buffer flushes, which typically render such traces to be of little (or no) value!

For our example measurement, scoring of the initial summary report in Section @ref start_workflow_filtering with the filter applied estimated a total memory requirement of 30 MiB per process (which could be verified by re-scoring the  filtered summary measurement).
As this exceeds the default `SCOREP_TOTAL_MEMORY` setting of 16 MiB, use of the prepared filtering file alone is not yet sufficient to avoid intermediate trace buffer flushes.
In addition, the `SCOREP_TOTAL_MEMORY` setting has to be adjusted accordingly before starting the trace collection and analysis.
For the example measurement shown below, a slightly larger memory buffer of 32 MiB is used, although this is not strictly necessary.
As an alternative, the filtering file could be extended to also exclude additional routines from measurement (e.g., `binvrhs`) to further reduce the trace buffer requirements at the expense of loosing details.

With this setting in place, a trace measurement can now be collected and subsequently analyzed by the Scalasca trace analyzer.
Note that renaming or removing the summary experiment directory is not necessary, as trace experiments are created with suffix &quot;`trace`&quot;.

@verbinclude start_trace.log

As can be seen, the `scalasca -analyze` convenience command automatically initiates the trace analysis after successful trace collection.
Besides the already known files `MANIFEST.md`, `scorep.cfg`, `scorep.filter`, and `scorep.log`, the generated experiment directory `scorep_bt_144_trace` now contains artifacts related to trace measurement and analysis:
 - an OTF2 trace archive consisting of the anchor file `traces.otf2`, the global definitions file `traces.def`, and the per-process data in the `traces/` directory,
 - the trace analysis log file `scout.log`, and finally
 - the trace analysis reports `scout.cubex` and `trace.stat`.

The Scalasca trace analyzer also warned about a number of point-to-point clock condition violations it detected.
A clock condition violation is a violation of the logical event order that can occur when the local clocks of the individual compute nodes are insufficiently synchronized.
For example, based on the measured timestamps, a receive operation may appear to have finished before the corresponding send operation started---something that is obviously not possible.
The Scalasca trace analyzer includes a correction algorithm @cite becker_ea:2009 that can be applied in such cases to restore the logical event order, while trying to preserve the length of intervals between local events in the vicinity of the violation.

To enable this correction algorithm, the `--time-correct` command-line option has to be passed to the Scalasca trace analyzer.
However, since the analyzer is implicitly started through the `scalasca -analyze` command, this option has to be set using the `SCAN_ANALYZE_OPTS` environment variable, which holds command-line options that `scalasca -analyze` should forward to the trace analyzer.
Instead of collecting and analyzing a new experiment, an existing trace measurement can also be re-analyzed using the `-a` option of the `scalasca -analyze` command:

@verbinclude start_trace_correct.log

@note
    The additional time required to execute the timestamp correction algorithm is typically small compared to the trace data I/O time and waiting times in the batch queue for starting a second analysis job.
    On platforms where clock condition violations are likely to occur (i.e., clusters), it is therefore often convenient to enable the timestamp correction algorithm by default.

Similar to the summary report, the trace analysis report can finally be post-processed and interactively explored using the Cube report browser, again using the `scalasca -examine` convenience command:

@verbinclude start_trace_exam.log

The report generated by the Scalasca trace analyzer is again a profile in CUBE4 format, however, enriched with additional performance properties.
Examination shows that about two-thirds of the time spent in MPI point-to-point communication is waiting time, split into roughly 60\% in <i>Late Sender</i> and 40\% in <i>Late Receiver</i> wait states (see Figure @ref TraceAnalysis).
While the execution time in the `solve_cell` routines looked relatively balanced in the summary profile, examination of the <i>Critical path imbalance</i> metric shows that these routines in fact exhibit a small amount of imbalance, which is likely to cause the wait states at the next synchronization point.
This can be verified using the <i>Late Sender delay costs</i> metric, which confirms that the `solve_cells` as well as the `y_backsubstitute` and `z_backsubstitute` routines are responsible for almost all of the <i>Late Sender</i> wait states.
Likewise, the <i>Late Receiver delay costs</i> metric shows that the majority of the <i>Late Receiver</i> wait states are caused by the `solve_cells` routines as well as the `MPI_Wait` calls in the solver routines, where the latter indicates a communication imbalance or an inefficient communication pattern.

@figure{start_trace.png,TraceAnalysis,Screenshot of a trace analysis result in the Cube report browser.,width=\textwidth}

@navigation_prev{start_workflow_summary}
*/
