<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<title>Performance properties</title>
</head>
<body>
<h2>Performance properties</h2>

<a name="time"><h3>Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Total time spent for program execution including the idle times of CPUs
reserved for worker threads during OpenMP sequential execution.  This
pattern assumes that every thread of a process allocated a separate CPU
during the entire runtime of the process.  Executions in a time-shared environment
will also include time slices used by other processes.  Over-subscription
of processor cores (e.g., exploiting hardware threads) will also manifest
as additional CPU allocation time.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree hierarchy to break down total time into
constituent parts which will help determine how much of it is due to
local/serial computation versus MPI, OpenMP, or POSIX thread parallelization
costs, and how much of that time is wasted waiting for other processes
or threads due to ineffective load balance or due to insufficient
parallelism.
</dd><p><dd>
Expand the call tree to identify important callpaths and routines where
most time is spent, and examine the times for each process or thread to
locate load imbalance.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#execution">Execution Time</a><br/>
    <a href="#overhead">Overhead Time</a><br/>
    <a href="#omp_idle_threads">OpenMP Idle Threads Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="visits"><h3>Visits</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of times a call path has been visited.  Visit counts for MPI
routine call paths directly relate to the number of MPI <a href="#comms">MPI Communication Operations</a> and
<a href="#syncs">MPI Synchronization Operations</a>.  Visit counts for OpenMP operations and parallel regions
(loops) directly relate to the number of times they were executed.
Routines which were not instrumented, or were filtered during measurement,
do not appear on recorded call paths.  Similarly, routines are not shown
if the compiler optimizer successfully in-lined them prior to automatic
instrumentation.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Call paths that are frequently visited (and thereby have high exclusive
Visit counts) can be expected to have an important role in application
execution performance (e.g., <a href="#execution">Execution Time</a>).  Very frequently executed
routines, which are relatively short and quick to execute, may have an
adverse impact on measurement quality.  This can be due to
instrumentation preventing in-lining and other compiler optimizations
and/or overheads associated with measurement such as reading timers and
hardware counters on routine entry and exit.  When such routines consist
solely of local/sequential computation (i.e., neither communication nor
synchronization), they should be eliminated to improve the quality of
the parallel measurement and analysis.  One approach is to specify the
names of such routines in a <em>filter</em> file for subsequent
measurements to ignore, and thereby considerably reduce their
measurement impact.  Alternatively, <em>selective instrumentation</em>
can be employed to entirely avoid instrumenting such routines and
thereby remove all measurement impact.  In both cases, uninstrumented
and filtered routines will not appear in the measurement and analysis,
much as if they had been "in-lined" into their calling routine.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="execution"><h3>Execution Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on program execution but without the idle times of worker
threads during OpenMP sequential execution and time spent on tasks
related to trace generation.  Includes time blocked in system calls
(e.g., waiting for I/O to complete) and processor stalls (e.g.,
memory accesses).
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A low fraction of execution time indicates a suboptimal measurement
configuration leading to trace buffer flushes (see <a href="#overhead">Overhead Time</a>) or
inefficient usage of the available hardware resources (see
<a href="#omp_idle_threads">OpenMP Idle Threads Time</a>).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#time">Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#comp">Computation Time</a><br/>
    <a href="#mpi">MPI Time</a><br/>
    <a href="#omp_time">OpenMP Time</a><br/>
    <a href="#pthread_time">POSIX Threads Time</a><br/>
    <a href="#openacc_time">OpenACC Time</a><br/>
    <a href="#opencl_time">OpenCL Time</a><br/>
    <a href="#cuda_time">CUDA Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="overhead"><h3>Overhead Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent performing major tasks related to measurement, such as
creation of the experiment archive directory, clock synchronization, or
dumping trace buffer contents to a file.  Note that normal per-event
overheads &ndash; such as event acquisition, reading timers and
hardware counters, runtime call-path summarization, and storage in trace
buffers &ndash; is <em>not</em> included.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Significant measurement overheads are typically incurred when
measurement is initialized (e.g., in the program <tt>main</tt> routine
or <tt>MPI_Init</tt>) and finalized (e.g., in <tt>MPI_Finalize</tt>),
and are generally unavoidable.  While they extend the total (wallclock)
time for measurement, when they occur before parallel execution starts
or after it completes, the quality of measurement of the parallel
execution is not degraded.  Trace file writing overhead time can be
kept to a minimum by specifying an efficient parallel filesystem (when
provided) for the experiment archive (e.g.,
<tt>SCOREP_EXPERIMENT_DIRECTORY=/work/mydir</tt>).
</dd><p><dd>
When measurement overhead is reported for other call paths, especially
during parallel execution, measurement perturbation is considerable and
interpretation of the resulting analysis much more difficult.  A common
cause of measurement overhead during parallel execution is the flushing
of full trace buffers to disk: warnings issued by the measurement
system indicate when this occurs.  When flushing occurs simultaneously
for all processes and threads, the associated perturbation is
localized in time.  More usually, buffer filling and flushing occurs
independently at different times on each process/thread and the
resulting perturbation is extremely disruptive, often forming a
catastrophic chain reaction.  It is highly advisable to avoid
intermediate trace buffer flushes by appropriate instrumentation and
measurement configuration, such as specifying a <em>filter</em> file
listing purely computational routines (classified as type USR by
<em>scorep-score&nbsp;-r</em>&nbsp;) or an adequate trace buffer size
(<tt>SCOREP_TOTAL_MEMORY</tt> larger than max_buf reported by
<em>scorep-score</em>).  If the maximum trace buffer capacity requirement
remains too large for a full-size measurement, it may be necessary to
configure the subject application with a smaller problem size or to
perform fewer iterations/timesteps to shorten the measurement (and
thereby reduce the size of the trace).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#time">Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comp"><h3>Computation Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in computational parts of the application, excluding
communication and synchronization overheads of parallelization
libaries/language extensions such as MPI, OpenMP, or POSIX threads.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the call tree to determine important callpaths and routines
where most computation time is spent, and examine the time for
each process or thread on those callpaths looking for significant
variations which might indicate the origin of load imbalance.
</dd><p><dd>
Where computation time on each process/thread is unexpectedly
slow, profiling with PAPI preset or platform-specific hardware counters
may help to understand the origin.  Serial program profiling tools
(e.g., gprof) may also be helpful.  Generally, compiler optimization
flags and optimized libraries should be investigated to improve serial
performance, and where necessary alternative algorithms employed.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#opencl_kernel_executions">OpenCL Kernel Time</a><br/>
    <a href="#cuda_kernel_executions">CUDA Kernel Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi"><h3>MPI Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in (instrumented) MPI calls.  Note that depending on the
setting of the <tt>SCOREP_MPI_ENABLE_GROUPS</tt> environment variable,
certain classes of MPI calls may have been excluded from measurement and
therefore do not show up in the analysis report.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree to determine which classes of MPI operation
contribute the most time.  Typically the remaining (exclusive) MPI Time,
corresponding to instrumented MPI routines that are not in one of the
child classes, will be negligible.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_management">MPI Management Time</a><br/>
    <a href="#mpi_synchronization">MPI Synchronization Time</a><br/>
    <a href="#mpi_communication">MPI Communication Time</a><br/>
    <a href="#mpi_io">MPI File I/O Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_management"><h3>MPI Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI calls related to management operations, such as MPI
initialization and finalization, opening/closing of files used for MPI
file I/O, or creation/deletion of various handles (e.g., communicators
or RMA windows).
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree to determine which classes of MPI management
operation contribute the most time.  While some management costs are
unavoidable, others can be decreased by improving load balance or reusing
existing handles rather than repeatedly creating and deleting them.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi">MPI Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_init_exit">MPI Init/Finalize Time</a><br/>
    <a href="#mpi_mgmt_comm">MPI Communicator Management Time</a><br/>
    <a href="#mpi_mgmt_file">MPI File Management Time</a><br/>
    <a href="#mpi_mgmt_win">MPI Window Management Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_init_exit"><h3>MPI Init/Finalize Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI initialization and finalization calls, i.e.,
<tt>MPI_Init</tt> or <tt>MPI_Init_thread</tt> and
<tt>MPI_Finalize</tt>.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
These are unavoidable one-off costs for MPI parallel programs, which
can be expected to increase for larger numbers of processes.  Some
applications may not use all of the processes provided (or not use some
of them for the entire execution), such that unused and wasted
processes wait in <tt>MPI_Finalize</tt> for the others to finish.  If
the proportion of time in these calls is significant, it is probably
more effective to use a smaller number of processes (or a larger amount
of computation).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_management">MPI Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_init_completion">MPI Initialization Completion Time</a><br/>
    <a href="#mpi_finalize_wait">Wait at MPI Finalize Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_init_completion"><h3>MPI Initialization Completion Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI initialization after the first process has left the
operation.
</dd><p><dd>
<br>
<div align="center">
<img src="BarrierCompletion.png" alt="MPI Initialization Completion Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Generally all processes can be expected to leave MPI initialization
simultaneously, and any significant initialization completion time may
indicate an inefficient MPI implementation or interference from other
processes running on the same compute resources.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_init_exit">MPI Init/Finalize Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_finalize_wait"><h3>Wait at MPI Finalize Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in front of MPI finalization, which is the time
inside <tt>MPI_Finalize</tt> until the last processes has reached
finalization.
</dd><p><dd>
<br>
<div align="center">
<img src="WaitAtBarrier.png" alt="MPI Wait at Finalize Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time at finalization can be an indication of load
imbalance.  Examine the waiting times for each process and try to
distribute the preceding computation from processes with the shortest
waiting times to those with the longest waiting times.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_init_exit">MPI Init/Finalize Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_mgmt_comm"><h3>MPI Communicator Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI Communicator management routines such as creating and
freeing communicators, Cartesian and graph topologies, and getting or
setting communicator attributes.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
There can be significant time in collective operations such as
<tt>MPI_Comm_create</tt>, <tt>MPI_Comm_free</tt> and
<tt>MPI_Cart_create</tt> that are considered neither explicit
synchronization nor communication, but result in implicit barrier
synchronization of participating processes.  Avoidable waiting time
for these operations will be reduced if all processes execute them
simultaneously.  If these are repeated operations, e.g., in a loop,
it is worth investigating whether their frequency can be reduced by
re-use.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_management">MPI Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_mgmt_file"><h3>MPI File Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI file management routines such as opening, closing,
deleting, or resizing files, seeking, syncing, and setting or retrieving
file parameters or the process's view of the data in the file.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Collective file management calls (see <a href="#mpi_file_cops">MPI Collective File Operations</a>) may suffer from
wait states due to load imbalance.  Examine the times spent in collective
management routines for each process and try to distribute the preceding
computation from processes with the shortest times to those with the
longest times.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_management">MPI Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_mgmt_win"><h3>MPI Window Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI window management routines such as creating and freeing
memory windows and getting or setting window attributes.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_management">MPI Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_wait_at_create">MPI Wait at Window Create Time</a><br/>
    <a href="#mpi_rma_wait_at_free">MPI Wait at Window Free Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_synchronization"><h3>MPI Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI explicit synchronization calls, such as barriers and
remote memory access window synchronization.  Time in point-to-point
message transfers with no payload data used for coordination is currently
part of <a href="#mpi_point2point">MPI Point-to-point Communication Time</a>.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree further to determine the proportion of time in
different classes of MPI synchronization operations.  Expand the
calltree to identify which callpaths are responsible for the most
synchronization time.  Also examine the distribution of synchronization
time on each participating process for indication of load imbalance in
preceding code.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi">MPI Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_sync_collective">MPI Collective Synchronization Time</a><br/>
    <a href="#mpi_rma_synchronization">MPI One-sided Synchronization Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_sync_collective"><h3>MPI Collective Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total time spent in MPI barriers.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
When the time for MPI explicit barrier synchronization is significant,
expand the call tree to determine which <tt>MPI_Barrier</tt> calls are
responsible, and compare with their <a href="#visits">Visits</a> count to see how
frequently they were executed.  Barrier synchronizations which are not
necessary for correctness should be removed.  It may also be appropriate
to use a communicator containing fewer processes, or a number of
point-to-point messages for coordination instead.  Also examine the
distribution of time on each participating process for indication of
load imbalance in preceding code.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_synchronization">MPI Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_barrier_wait">Wait at MPI Barrier Time</a><br/>
    <a href="#mpi_barrier_completion">MPI Barrier Completion Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_barrier_wait"><h3>Wait at MPI Barrier Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in front of an MPI barrier, which is the time inside
the barrier call until the last processes has reached the barrier.
</dd><p><dd>
<br>
<div align="center">
<img src="WaitAtBarrier.png" alt="MPI Wait at Barrier Example">
</div>
<br>

</dd><p><dd>
Note that Scalasca does not yet analyze non-blocking barriers introduced
with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time at barriers can be an indication of load
imbalance.  Examine the waiting times for each process and try to
distribute the preceding computation from processes with the shortest
waiting times to those with the longest waiting times.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_sync_collective">MPI Collective Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_barrier_completion"><h3>MPI Barrier Completion Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI barriers after the first process has left the operation.
</dd><p><dd>
<br>
<div align="center">
<img src="BarrierCompletion.png" alt="MPI Barrier Completion Example">
</div>
<br>

</dd><p><dd>
Note that Scalasca does not yet analyze non-blocking barriers introduced
with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Generally all processes can be expected to leave MPI barriers
simultaneously, and any significant barrier completion time may
indicate an inefficient MPI implementation or interference from other
processes running on the same compute resources.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_sync_collective">MPI Collective Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_communication"><h3>MPI Communication Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI communication calls, including point-to-point,
collective, and one-sided communication.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree further to determine the proportion of time in
different classes of MPI communication operations.  Expand the calltree
to identify which callpaths are responsible for the most communication
time.  Also examine the distribution of communication time on each
participating process for indication of communication imbalance or load
imbalance in preceding code.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi">MPI Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_point2point">MPI Point-to-point Communication Time</a><br/>
    <a href="#mpi_collective">MPI Collective Communication Time</a><br/>
    <a href="#mpi_rma_communication">MPI One-sided Communication Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_point2point"><h3>MPI Point-to-point Communication Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total time spent in MPI point-to-point communication calls.  Note that
this is only the respective times for the sending and receiving calls,
and <em>not</em> message transmission time.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Investigate whether communication time is commensurate with the number
of <a href="#comms">MPI Communication Operations</a> and <a href="#bytes">MPI Bytes Transferred</a>.  Consider replacing blocking
communication with non-blocking communication that can potentially be
overlapped with computation, or using persistent communication to
amortize message setup costs for common transfers.  Also consider the
mapping of processes onto compute resources, especially if there are
notable differences in communication time for particular processes,
which might indicate longer/slower transmission routes or network
congestion.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_communication">MPI Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_latesender">MPI Late Sender Time</a><br/>
    <a href="#mpi_latereceiver">MPI Late Receiver Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_latesender"><h3>MPI Late Sender Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Refers to the time lost waiting caused by a blocking receive operation
(e.g., <tt>MPI_Recv</tt> or <tt>MPI_Wait</tt>) that is posted earlier
than the corresponding send operation.
</dd><p><dd>
<br>
<div align="center">
<img src="LateSender.png" alt="MPI Late Sender Example">
</div>
<br>

</dd><p><dd>
If the receiving process is waiting for multiple messages to arrive
(e.g., in an call to <tt>MPI_Waitall</tt>), the maximum waiting time is
accounted, i.e., the waiting time due to the latest sender.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Try to replace <tt>MPI_Recv</tt> with a non-blocking receive <tt>MPI_Irecv</tt>
that can be posted earlier, proceed concurrently with computation, and
complete with a wait operation after the message is expected to have been
sent.  Try to post sends earlier, such that they are available when
receivers need them.  Note that outstanding messages (i.e., sent before the
receiver is ready) will occupy internal message buffers, and that large
numbers of posted receive buffers will also introduce message management
overhead, therefore moderation is advisable.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_point2point">MPI Point-to-point Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_latesender_wo">MPI Late Sender, Wrong Order Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_latesender_wo"><h3>MPI Late Sender, Wrong Order Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
A Late Sender situation may be the result of messages that are received
in the wrong order.  If a process expects messages from one or more
processes in a certain order, although these processes are sending them
in a different order, the receiver may need to wait for a message if it
tries to receive a message early that has been sent late.
</dd><p><dd>
This pattern comes in two variants:
<ul>
    <li>The messages involved were sent from the same source location</li>
    <li>The messages involved were sent from different source locations</li>
</ul>
See the description of the corresponding specializations for more details.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Check the proportion of <a href="#comms_recv">MPI Point-to-point Receive Communication Operations</a> that are <a href="#mpi_cls_count">MPI Late Sender Instances (Communications)</a>.
Swap the order of receiving from different sources to match the most
common ordering.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_latesender">MPI Late Sender Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_lswo_different">MPI Late Sender, Wrong Order Time / Different Sources</a><br/>
    <a href="#mpi_lswo_same">MPI Late Sender, Wrong Order Time / Same Source</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_lswo_different"><h3>MPI Late Sender, Wrong Order Time / Different Sources</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
This specialization of the <i>Late Sender, Wrong Order</i> pattern refers
to wrong order situations due to messages received from different source
locations.
</dd><p><dd>
<br>
<div align="center">
<img src="LSWO_DifferentSource.png" alt="MPI Late Sender, wrong order (different source) Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Check the proportion of <a href="#comms_recv">MPI Point-to-point Receive Communication Operations</a> that are
<a href="#mpi_clswo_count">MPI Late Sender, Wrong Order Instances (Communications)</a>.  Swap the order of receiving from different
sources to match the most common ordering.  Consider using the wildcard
<tt>MPI_ANY_SOURCE</tt> to receive (and process) messages as they
arrive from any source rank.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_latesender_wo">MPI Late Sender, Wrong Order Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_lswo_same"><h3>MPI Late Sender, Wrong Order Time / Same Source</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
This specialization of the <i>Late Sender, Wrong Order</i> pattern refers
to wrong order situations due to messages received from the same source
location.
</dd><p><dd>
<br>
<div align="center">
<img src="LSWO_SameSource.png" alt="MPI Late Sender, wrong order (same source) Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Swap the order of receiving to match the order messages are sent, or
swap the order of sending to match the order they are expected to be
received.  Consider using the wildcard <tt>MPI_ANY_TAG</tt> to receive
(and process) messages in the order they arrive from the source.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_latesender_wo">MPI Late Sender, Wrong Order Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_latereceiver"><h3>MPI Late Receiver Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
A send operation may be blocked until the corresponding receive
operation is called.  This pattern refers to the time spent waiting
as a result of this situation.
</dd><p><dd>
<br>
<div align="center">
<img src="LateReceiver.png" alt="MPI Late Receiver Example">
</div>
<br>

</dd><p><dd>
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Check the proportion of <a href="#comms_send">MPI Point-to-point Send Communication Operations</a> that are <a href="#mpi_clr_count">MPI Late Receiver Instances (Communications)</a>.
The MPI implementation may be working in synchronous mode by default,
such that explicit use of asynchronous nonblocking sends can be tried.
If the size of the message to be sent exceeds the available MPI
internal buffer space then the operation will be blocked until the data
can be transferred to the receiver: some MPI implementations allow
larger internal buffers or different thresholds to be specified.  Also
consider the mapping of processes onto compute resources, especially if
there are notable differences in communication time for particular
processes, which might indicate longer/slower transmission routes or
network congestion.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_point2point">MPI Point-to-point Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_collective"><h3>MPI Collective Communication Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total time spent in MPI collective communication calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
As the number of participating MPI processes
increase (i.e., ranks in <tt>MPI_COMM_WORLD</tt> or a subcommunicator),
time in collective communication can be expected to increase
correspondingly.  Part of the increase will be due to additional data
transmission requirements, which are generally similar for all
participants.  A significant part is typically time some (often many)
processes are blocked waiting for the last of the required participants
to reach the collective operation.  This may be indicated by significant
variation in collective communication time across processes, but is
most conclusively quantified from the child metrics determinable via
automatic trace pattern analysis.
</dd><p><dd>
Since basic transmission cost per byte for collectives can be relatively high,
combining several collective operations of the same type each with small amounts of data
(e.g., a single value per rank) into fewer operations with larger payloads
using either a vector/array of values or aggregate datatype may be beneficial.
(Overdoing this and aggregating very large message payloads is counter-productive
due to explicit and implicit memory requirements, and MPI protocol switches
for messages larger than an eager transmission threshold.)
</dd><p><dd>
MPI implementations generally provide optimized collective communication operations,
however, in rare cases, it may be appropriate to replace a collective
communication operation provided by the MPI implementation with a
customized implementation of your own using point-to-point operations.
For example, certain MPI implementations of <tt>MPI_Scan</tt> include
unnecessary synchronization of all participating processes, or
asynchronous variants of collective operations may be preferable to
fully synchronous ones where they permit overlapping of computation.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_communication">MPI Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_earlyreduce">MPI Early Reduce Time</a><br/>
    <a href="#mpi_earlyscan">MPI Early Scan Time</a><br/>
    <a href="#mpi_latebroadcast">MPI Late Broadcast Time</a><br/>
    <a href="#mpi_wait_nxn">MPI Wait at N x N Time</a><br/>
    <a href="#mpi_nxn_completion">MPI N x N Completion Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_earlyreduce"><h3>MPI Early Reduce Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Collective communication operations that send data from all processes
to one destination process (i.e., n-to-1) may suffer from waiting times
if the destination process enters the operation earlier than any of its
sending counterparts.  This pattern refers to the time lost on the root
rank as a result of this situation, accounting for the waiting time due
to the latest sending process.  It applies to the MPI calls
<tt>MPI_Reduce</tt>, <tt>MPI_Gather</tt> and <tt>MPI_Gatherv</tt>.
</dd><p><dd>
<br>
<div align="center">
<img src="EarlyReduce.png" alt="MPI Early Reduce Example">
</div>
<br>

</dd><p><dd>
Note that Scalasca does not yet analyze non-blocking collectives introduced
with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_collective">MPI Collective Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_earlyscan"><h3>MPI Early Scan Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
<tt>MPI_Scan</tt> or <tt>MPI_Exscan</tt> operations may suffer from
waiting times if the process with rank <i>n</i> enters the operation
earlier than its sending counterparts (i.e., ranks 0..<i>n</i>-1).  This
pattern refers to the time lost as a result of this situation.
</dd><p><dd>
<br>
<div align="center">
<img src="EarlyScan.png" alt="MPI Early Scan Example">
</div>
<br>

</dd><p><dd>
Note that Scalasca does not yet analyze non-blocking collectives introduced
with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_collective">MPI Collective Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_latebroadcast"><h3>MPI Late Broadcast Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Collective communication operations that send data from one source
process to all processes (i.e., 1-to-n) may suffer from waiting times
if destination processes enter the operation earlier than the source
process, that is, before any data could have been sent.  This pattern
refers to the time lost as a result of this situation.  It applies to
the MPI calls <tt>MPI_Bcast</tt>, <tt>MPI_Scatter</tt> and
<tt>MPI_Scatterv</tt>.
</dd><p><dd>
<br>
<div align="center">
<img src="LateBroadcast.png" alt="MPI Late Broadcast Example">
</div>
<br>

</dd><p><dd>
Note that Scalasca does not yet analyze non-blocking collectives introduced
with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_collective">MPI Collective Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_nxn"><h3>MPI Wait at N x N Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Collective communication operations that send data from all processes
to all processes (i.e., n-to-n) exhibit an inherent synchronization
among all participants, that is, no process can finish the operation
until the last process has started it.  This pattern covers the time
spent in n-to-n operations until all processes have reached it.  It
applies to the MPI calls <tt>MPI_Reduce_scatter</tt>,
<tt>MPI_Reduce_scatter_block</tt>, <tt>MPI_Allgather</tt>,
<tt>MPI_Allgatherv</tt>, <tt>MPI_Allreduce</tt> and <tt>MPI_Alltoall</tt>.
</dd><p><dd>
<br>
<div align="center">
<img src="WaitAtNxN.png" alt="MPI Wait at N x N Example">
</div>
<br>

</dd><p><dd>
Note that the time reported by this pattern is not necessarily
completely waiting time since some processes could &ndash; at least
theoretically &ndash; already communicate with each other while others
have not yet entered the operation.
</dd><p><dd>
Also note that Scalasca does not yet analyze non-blocking and neighborhood
collectives introduced with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_collective">MPI Collective Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_nxn_completion"><h3>MPI N x N Completion Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
This pattern refers to the time spent in MPI n-to-n collectives after
the first process has left the operation.
</dd><p><dd>
<br>
<div align="center">
<img src="NxNCompletion.png" alt="MPI N x N Completion Example">
</div>
<br>

</dd><p><dd>
Note that the time reported by this pattern is not necessarily
completely waiting time since some processes could &ndash; at least
theoretically &ndash; still communicate with each other while others
have already finished communicating and exited the operation.
</dd><p><dd>
Also note that Scalasca does not yet analyze non-blocking and neighborhood
collectives introduced with MPI v3.0.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_collective">MPI Collective Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_io"><h3>MPI File I/O Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI file I/O calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree further to determine the proportion of time in
different classes of MPI file I/O operations.  Expand the calltree to
identify which callpaths are responsible for the most file I/O time.
Also examine the distribution of MPI file I/O time on each process for
indication of load imbalance.  Use a parallel filesystem (such as
<tt>/work</tt>) when possible, and check that appropriate hints values
have been associated with the <tt>MPI_Info</tt> object of MPI files.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi">MPI Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_io_individual">MPI Individual File I/O Time</a><br/>
    <a href="#mpi_io_collective">MPI Collective File I/O Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_io_individual"><h3>MPI Individual File I/O Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in individual MPI file I/O calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to identify which callpaths are responsible for the
most individual file I/O time.  When multiple processes read and write
to files, MPI collective file reads and writes can be more efficient.
Examine the number of <a href="#mpi_file_irops">MPI Individual File Read Operations</a> and <a href="#mpi_file_iwops">MPI Individual File Write Operations</a> to
locate potential opportunities for collective I/O.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_io">MPI File I/O Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_io_collective"><h3>MPI Collective File I/O Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in collective MPI file I/O calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to identify which callpaths are responsible for the
most collective file I/O time.  Examine the distribution of times on
each participating process for indication of imbalance in the operation
itself or in preceding code.  Examine the number of <a href="#mpi_file_crops">MPI Collective File Read Operations</a>
and <a href="#mpi_file_cwops">MPI Collective File Write Operations</a> done by each process as a possible origin of
imbalance.  Where asychrony or imbalance prevents effective use of
collective file I/O, individual (i.e., non-collective) file I/O may be
preferable.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_io">MPI File I/O Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_idle_threads"><h3>OpenMP Idle Threads Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Idle time on CPUs that may be reserved for teams of threads when the
process is executing sequentially before and after OpenMP parallel
regions, or with less than the full team within OpenMP parallel
regions.
</dd><p><dd>
<br>
<div align="center">
<img src="OMPIdle.png" alt="OMP Idle Threads Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
On shared compute resources, unused threads may simply sleep and allow
the resources to be used by other applications, however, on dedicated
compute resources (or where unused threads busy-wait and thereby occupy
the resources) their idle time is charged to the application.
According to Amdahl's Law, the fraction of inherently serial execution
time limits the effectiveness of employing additional threads to reduce
the execution time of parallel regions.  Where the Idle Threads Time is
significant, total <a href="#time">Time</a> (and wall-clock execution time) may be
reduced by effective parallelization of sections of code which execute
serially.  Alternatively, the proportion of wasted Idle Threads Time
will be reduced by running with fewer threads, albeit resulting in a
longer wall-clock execution time but more effective usage of the
allocated compute resources.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#time">Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_limited_parallelism">OpenMP Limited Parallelism Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_limited_parallelism"><h3>OpenMP Limited Parallelism Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Idle time on CPUs that may be reserved for threads within OpenMP
parallel regions where not all of the thread team participates.
</dd><p><dd>
<br>
<div align="center">
<img src="OMPLimitedParallelism.png" alt="OMP Limited parallelism Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Code sections marked as OpenMP parallel regions which are executed
serially (i.e., only by the master thread) or by less than the full
team of threads, can result in allocated but unused compute resources
being wasted.  Typically this arises from insufficient work being
available within the marked parallel region to productively employ all
threads.  This may be because the loop contains too few iterations or
the OpenMP runtime has determined that additional threads would not be
productive.  Alternatively, the OpenMP <tt>omp_set_num_threads</tt> API
or <tt>num_threads</tt> or <tt>if</tt> clauses may have been explicitly
specified, e.g., to reduce parallel execution overheads such as
<a href="#omp_management">OpenMP Thread Management Time</a> or <a href="#omp_synchronization">OpenMP Synchronization Time</a>.  If the proportion of
OpenMP Limited Parallelism Time is significant, it may be more
efficient to run with fewer threads for that problem size.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_idle_threads">OpenMP Idle Threads Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_time"><h3>OpenMP Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in OpenMP API calls and code generated by the OpenMP compiler.
In particular, this includes thread team management and synchronization
activities.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree to determine which classes of OpenMP activities
contribute the most time.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_management">OpenMP Thread Management Time</a><br/>
    <a href="#omp_synchronization">OpenMP Synchronization Time</a><br/>
    <a href="#omp_flush">OpenMP Flush Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_management"><h3>OpenMP Thread Management Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent managing teams of threads, creating and initializing them
when forking a new parallel region and clearing up afterwards when
joining.
</dd><p><dd>
<br>
<div align="center">
<img src="OMPThreadManagement.png" alt="OMP Management Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Management overhead for an OpenMP parallel region depends on the number
of threads to be employed and the number of variables to be initialized
and saved for each thread, each time the parallel region is executed.
Typically a pool of threads is used by the OpenMP runtime system to
avoid forking and joining threads in each parallel region, however,
threads from the pool still need to be added to the team and assigned
tasks to perform according to the specified schedule.  When the overhead
is a significant proportion of the time for executing the parallel
region, it is worth investigating whether several parallel regions can
be combined to amortize thread management overheads.  Alternatively, it
may be appropriate to reduce the number of threads either for the
entire execution or only for this parallel region (e.g., via
<tt>num_threads</tt> or <tt>if</tt> clauses).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_time">OpenMP Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_fork">OpenMP Thread Team Fork Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_fork"><h3>OpenMP Thread Team Fork Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent creating and initializing teams of threads.
</dd><p><dd>
<br>
<div align="center">
<img src="OMPThreadFork.png" alt="OMP Fork Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_management">OpenMP Thread Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_synchronization"><h3>OpenMP Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in OpenMP synchronization, whether barriers or mutual exclusion
via ordered sequentialization, critical sections, atomics or lock API calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_time">OpenMP Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_barrier">OpenMP Barrier Synchronization Time</a><br/>
    <a href="#omp_critical">OpenMP Critical Synchronization Time</a><br/>
    <a href="#omp_lock_api">OpenMP Lock API Synchronization Time</a><br/>
    <a href="#omp_ordered">OpenMP Ordered Synchronization Time</a><br/>
    <a href="#omp_taskwait">OpenMP Taskwait Synchronization Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_barrier"><h3>OpenMP Barrier Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in implicit (compiler-generated) or explicit (user-specified)
OpenMP barrier synchronization.  Note that during measurement implicit
barriers are treated similar to explicit ones.  The instrumentation
procedure replaces an implicit barrier with an explicit barrier enclosed
by the parallel construct.  This is done by adding a <tt>nowait</tt>
clause and a barrier directive as the last statement of the parallel
construct.  In cases where the implicit barrier cannot be removed (i.e.,
parallel region), the explicit barrier is executed in front of the
implicit barrier, which will then be negligible because the thread team
will already be synchronized when reaching it.  The synthetic explicit
barrier appears as a special implicit barrier construct.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_synchronization">OpenMP Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_ebarrier">OpenMP Explicit Barrier Synchronization Time</a><br/>
    <a href="#omp_ibarrier">OpenMP Implicit Barrier Synchronization Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_ebarrier"><h3>OpenMP Explicit Barrier Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in explicit (i.e., user-specified) OpenMP <tt>barrier</tt>
synchronization, both waiting for other threads <a href="#omp_ebarrier_wait">Wait at Explicit OpenMP Barrier Time</a>
and inherent barrier processing overhead.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly barrier synchronizations and determine whether
they are necessary to ensure correctness or could be safely removed
(based on algorithm analysis).  Consider replacing an explicit barrier
with a potentially more efficient construct, such as a critical section
or atomic, or use explicit locks.  Examine the time that each thread
spends waiting at each explicit barrier, and try to re-distribute
preceding work to improve load balance.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_barrier">OpenMP Barrier Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_ebarrier_wait">Wait at Explicit OpenMP Barrier Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_ebarrier_wait"><h3>Wait at Explicit OpenMP Barrier Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in explicit (i.e., user-specified) OpenMP <tt>barrier</tt>
synchronization waiting for the last thread.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time at barriers can be an indication of load
imbalance.  Examine the waiting times for each thread and try to
distribute the preceding computation from threads with the shortest
waiting times to those with the longest waiting times.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_ebarrier">OpenMP Explicit Barrier Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_ibarrier"><h3>OpenMP Implicit Barrier Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in implicit (i.e., compiler-generated) OpenMP barrier
synchronization, both waiting for other threads <a href="#omp_ibarrier_wait">Wait at Implicit OpenMP Barrier Time</a>
and inherent barrier processing overhead.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the time that each thread spends waiting at each implicit
barrier, and if there is a significant imbalance then investigate
whether a <tt>schedule</tt> clause is appropriate.  Note that
<tt>dynamic</tt> and <tt>guided</tt> schedules may require more
<a href="#omp_management">OpenMP Thread Management Time</a> than <tt>static</tt> schedules.  Consider whether
it is possible to employ the <tt>nowait</tt> clause to reduce the
number of implicit barrier synchronizations.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_barrier">OpenMP Barrier Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_ibarrier_wait">Wait at Implicit OpenMP Barrier Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_ibarrier_wait"><h3>Wait at Implicit OpenMP Barrier Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in implicit (i.e., compiler-generated) OpenMP barrier
synchronization.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time at barriers can be an indication of load
imbalance.  Examine the waiting times for each thread and try to
distribute the preceding computation from threads with the shortest
waiting times to those with the longest waiting times.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_ibarrier">OpenMP Implicit Barrier Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_critical"><h3>OpenMP Critical Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting to enter OpenMP critical sections and in atomics,
where mutual exclusion restricts access to a single thread at a time.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly critical sections and atomics and determine
whether they are necessary to ensure correctness or could be safely
removed (based on algorithm analysis).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_synchronization">OpenMP Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_lock_contention_critical">OpenMP Critical Contention Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_lock_contention_critical"><h3>OpenMP Critical Contention Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The time lost waiting before entering a critical section while another
thread is still inside the section.
</dd><p><dd>
<br>
<div align="center">
<img src="LockContention.png" alt="OMP Critical Contention Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time can be an indication of too much balance,
since all threads arrive at the critical almost at the same time.  Examine
the waiting times for each thread and try to distribute the preceding
computation on the threads to allow a staggered arrival at the critical
section.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_critical">OpenMP Critical Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_lock_api"><h3>OpenMP Lock API Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in OpenMP lock API calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly usage of locks and determine whether they are
necessary to ensure correctness or could be safely removed (based on
algorithm analysis).  Consider re-writing the algorithm to use lock-free
data structures.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_synchronization">OpenMP Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#omp_lock_contention_api">OpenMP Lock API Contention Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_lock_contention_api"><h3>OpenMP Lock API Contention Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The time lost waiting for an explicit lock to be acquired while another
thread still holds the corresponding lock.
</dd><p><dd>
<br>
<div align="center">
<img src="LockContention.png" alt="OMP Lock API Contention Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time can be an indication of too much balance,
since all threads try to acquire the lock almost at the same time. Examine
the waiting times for each thread and try to distribute the preceding
computation on the threads to allow a staggered arrival at the lock.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_lock_api">OpenMP Lock API Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_ordered"><h3>OpenMP Ordered Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting to enter OpenMP <tt>ordered</tt> regions due to enforced
sequentialization of loop iteration execution order in the region.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly <tt>ordered</tt> regions and determine
whether they are necessary to ensure correctness or could be safely
removed (based on algorithm analysis).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_synchronization">OpenMP Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_taskwait"><h3>OpenMP Taskwait Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in OpenMP <tt>taskwait</tt> directives, waiting for child tasks
to finish.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_synchronization">OpenMP Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="omp_flush"><h3>OpenMP Flush Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in OpenMP <tt>flush</tt> directives.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#omp_time">OpenMP Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_time"><h3>POSIX Threads Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in instrumented POSIX threads API calls.  In particular, this
includes thread management and synchronization activities.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree to determine which classes of POSIX thread
activities contribute the most time.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#pthread_management">POSIX Threads Management Time</a><br/>
    <a href="#pthread_synchronization">POSIX Threads Synchronization Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_management"><h3>POSIX Threads Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent managing (i.e., creating, joining, cancelling, etc.) POSIX
threads.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Excessive POSIX threads management time in <tt>pthread_join</tt>
indicates load imbalance which causes wait states in the joining
threads waiting for the other thread to finish.  Examine the join
times and try to re-distribute the computation in the corresponding
worker threads to achieve a better load balance.
</dd><p><dd>
Also, correlate the thread management time to the <a href="#visits">Visits</a> of
management routines.  If visit counts are high, consider using a
thread pool to reduce the number of thread management operations.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#pthread_time">POSIX Threads Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_synchronization"><h3>POSIX Threads Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in POSIX threads synchronization calls, i.e., mutex and
condition variable operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree further to determine the proportion of time in
different classes of POSIX thread synchronization operations.  Expand the
calltree to identify which callpaths are responsible for the most
synchronization time.  Also examine the distribution of synchronization
time on each participating thread for indication of lock contention
effects.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#pthread_time">POSIX Threads Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#pthread_lock_api">POSIX Threads Mutex API Synchronization Time</a><br/>
    <a href="#pthread_conditional">POSIX Threads Condition API Synchronization Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_lock_api"><h3>POSIX Threads Mutex API Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in POSIX threads mutex API calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly usage of mutex operations and determine whether
they are necessary to ensure correctness or could be safely removed
(based on algorithm analysis).  Consider re-writing the algorithm to
use lock-free data structures.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#pthread_synchronization">POSIX Threads Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#pthread_lock_contention_mutex_lock">POSIX Threads Mutex Contention Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_lock_contention_mutex_lock"><h3>POSIX Threads Mutex Contention Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time lost waiting for a mutex lock to be acquired while another thread
still holds the corresponding lock.
</dd><p><dd>
<br>
<div align="center">
<img src="LockContention.png" alt="Pthread Mutex Lock Contention Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time in mutex locks can be an indication of too
much balance, since many threads try to acquire locks almost at the same
time.  Examine the waiting times for each thread and try to distribute
the preceding computation on the threads to allow a staggered arrival at
the lock API call.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#pthread_lock_api">POSIX Threads Mutex API Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_conditional"><h3>POSIX Threads Condition API Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in POSIX threads condition API calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly usage of condition operations and determine whether
they are necessary to ensure correctness or could be safely removed (based
on algorithm analysis).  Consider re-writing the algorithm to use data
structures without the need for condition variables.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#pthread_synchronization">POSIX Threads Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#pthread_lock_contention_conditional">POSIX Threads Condition Contention Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="pthread_lock_contention_conditional"><h3>POSIX Threads Condition Contention Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time lost waiting for a mutex lock to be acquired in a condition API
call while another thread still holds the corresponding lock.
</dd><p><dd>
<br>
<div align="center">
<img src="LockContention.png" alt="Pthread Condition Lock Contention Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large amount of waiting time in condition operations can be an indication
of imbalance.  Examine the waiting times for each thread and try to
distribute the preceding computation, in particular the work of the
threads responsible for fulfilling the condition.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#pthread_conditional">POSIX Threads Condition API Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="openacc_time"><h3>OpenACC Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in the OpenACC run-time system, API calls and on device.
If the OpenACC implementation is based on CUDA, and OpenACC and CUDA
support are both enabled during measurement, the CUDA activities from
within OpenACC will be accounted separately (just like CUDA calls
within MPI and other metric hierarchies).
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#openacc_setup">OpenACC Initialization/Finalization Time</a><br/>
    <a href="#openacc_comm">OpenACC Memory Management Time</a><br/>
    <a href="#openacc_sync">OpenACC Synchronization Time</a><br/>
    <a href="#openacc_kernel_launches">OpenACC Kernel Launch Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="openacc_setup"><h3>OpenACC Initialization/Finalization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time needed to initialize and finalize OpenACC and OpenACC kernels.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#openacc_time">OpenACC Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="openacc_comm"><h3>OpenACC Memory Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on memory management including data transfer from host to
device and vice versa.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#openacc_time">OpenACC Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="openacc_sync"><h3>OpenACC Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on OpenACC synchronization.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#openacc_time">OpenACC Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="openacc_kernel_launches"><h3>OpenACC Kernel Launch Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent to launch OpenACC kernels.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#openacc_time">OpenACC Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="opencl_kernel_executions"><h3>OpenCL Kernel Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent executing OpenCL kernels.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comp">Computation Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="opencl_time"><h3>OpenCL Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in the OpenCL run-time system, API and on device.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#opencl_setup">OpenCL General Management Time</a><br/>
    <a href="#opencl_comm">OpenCL Memory Management Time</a><br/>
    <a href="#opencl_sync">OpenCL Synchronization Time</a><br/>
    <a href="#opencl_kernel_launches">OpenCL Kernel Launch Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="opencl_setup"><h3>OpenCL General Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time needed for general OpenCL setup, e.g. initialization, device and
event control, etc.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#opencl_time">OpenCL Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="opencl_comm"><h3>OpenCL Memory Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on memory management including data transfer from host to
device and vice versa.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#opencl_time">OpenCL Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="opencl_sync"><h3>OpenCL Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on OpenCL synchronization.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#opencl_time">OpenCL Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="opencl_kernel_launches"><h3>OpenCL Kernel Launch Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent to launch OpenCL kernels.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#opencl_time">OpenCL Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="cuda_kernel_executions"><h3>CUDA Kernel Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent executing CUDA kernels.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comp">Computation Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="cuda_time"><h3>CUDA Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in the CUDA run-time system, API calls and on device.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#execution">Execution Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#cuda_setup">CUDA General Management Time</a><br/>
    <a href="#cuda_comm">CUDA Memory Management Time</a><br/>
    <a href="#cuda_sync">CUDA Synchronization Time</a><br/>
    <a href="#cuda_kernel_launches">CUDA Kernel Launch Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="cuda_setup"><h3>CUDA General Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time needed for general CUDA setup, e.g. initialization, control of
version, device, primary context, context, streams, events, occupancy,
etc.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#cuda_time">CUDA Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="cuda_comm"><h3>CUDA Memory Management Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on memory management including data transfer from host to
device and vice versa. Note that "memset" operations are considered
in <a href="#cuda_kernel_launches">CUDA Kernel Launch Time</a>.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#cuda_time">CUDA Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="cuda_sync"><h3>CUDA Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent on CUDA synchronization.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#cuda_time">CUDA Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="cuda_kernel_launches"><h3>CUDA Kernel Launch Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent to launch CUDA kernels, including "memset" operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#cuda_time">CUDA Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs"><h3>MPI Synchronization Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of MPI synchronization operations
that were executed.  This not only includes barrier calls, but also
communication operations which transfer no data (i.e., zero-sized
messages are considered to be used for coordination synchronization).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#syncs_p2p">MPI Point-to-point Synchronization Operations</a><br/>
    <a href="#syncs_coll">MPI Collective Synchronizations</a><br/>
    <a href="#syncs_rma">MPI One-sided Synchronization Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_p2p"><h3>MPI Point-to-point Synchronization Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI point-to-point synchronization operations, i.e.,
point-to-point transfers of zero-sized messages used for coordination.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate the most costly synchronizations and determine whether they are
necessary to ensure correctness or could be safely removed (based on
algorithm analysis).
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs">MPI Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#syncs_send">MPI Point-to-point Send Synchronization Operations</a><br/>
    <a href="#syncs_recv">MPI Point-to-point Receive Synchronization Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_send"><h3>MPI Point-to-point Send Synchronization Operations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of MPI point-to-point synchronization operations sending
a zero-sized message.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs_p2p">MPI Point-to-point Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_slr_count">MPI Late Receiver Instances (Synchronizations)</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_recv"><h3>MPI Point-to-point Receive Synchronization Operations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of MPI point-to-point synchronization operations receiving
a zero-sized message.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs_p2p">MPI Point-to-point Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_sls_count">MPI Late Sender Instances (Synchronizations)</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_coll"><h3>MPI Collective Synchronizations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of MPI collective synchronization operations.  This
does not only include barrier calls, but also calls to collective
communication operations that are neither sending nor receiving any
data.  Each process participating in the operation is counted, as
defined by the associated MPI communicator.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate synchronizations with the largest <a href="#mpi_sync_collective">MPI Collective Synchronization Time</a> and
determine whether they are necessary to ensure correctness or could be
safely removed (based on algorithm analysis).  Collective communication
operations that neither send nor receive data, yet are required for
synchronization, can be replaced with the more efficient
<tt>MPI_Barrier</tt>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs">MPI Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_rma"><h3>MPI One-sided Synchronization Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided synchronization operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs">MPI Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#syncs_rma_active">MPI One-sided Active Target Synchronization Operations</a><br/>
    <a href="#syncs_rma_passive">MPI One-sided Passive Target Synchronization Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_rma_active"><h3>MPI One-sided Active Target Synchronization Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided active target synchronization operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs_rma">MPI One-sided Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="syncs_rma_passive"><h3>MPI One-sided Passive Target Synchronization Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided passive target synchronization operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs_rma">MPI One-sided Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms"><h3>MPI Communication Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI communication operations, excluding calls transferring
no payload data (which are considered <a href="#syncs">MPI Synchronization Operations</a>).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#comms_p2p">MPI Point-to-point Communication Operations</a><br/>
    <a href="#comms_coll">MPI Collective Communications</a><br/>
    <a href="#comms_rma">MPI One-sided Communication Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_p2p"><h3>MPI Point-to-point Communication Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI point-to-point communication operations, excluding
calls transferring zero-sized messages.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms">MPI Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#comms_send">MPI Point-to-point Send Communication Operations</a><br/>
    <a href="#comms_recv">MPI Point-to-point Receive Communication Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_send"><h3>MPI Point-to-point Send Communication Operations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of MPI point-to-point send operations, excluding calls transferring
zero-sized messages.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_p2p">MPI Point-to-point Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_clr_count">MPI Late Receiver Instances (Communications)</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_recv"><h3>MPI Point-to-point Receive Communication Operations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of MPI point-to-point receive operations, excluding calls
transferring zero-sized messages.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_p2p">MPI Point-to-point Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_cls_count">MPI Late Sender Instances (Communications)</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_coll"><h3>MPI Collective Communications</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of MPI collective communication operations, excluding
calls neither sending nor receiving any data.  Each process participating
in the operation is counted, as defined by the associated MPI communicator.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Locate operations with the largest <a href="#mpi_collective">MPI Collective Communication Time</a> and compare
<a href="#bytes_coll">MPI Collective Bytes Transferred</a>.  Where multiple collective operations of the same type
are used in series with single values or small payloads, aggregation may
be beneficial in amortizing transfer overhead.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms">MPI Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#comms_cxch">MPI Collective Exchange Communications</a><br/>
    <a href="#comms_csrc">MPI Collective Communications as Source</a><br/>
    <a href="#comms_cdst">MPI Collective Communications as Destination</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_cxch"><h3>MPI Collective Exchange Communications</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of MPI collective communication operations which are both
sending and receiving data.  In addition to all-to-all and scan operations,
root processes of certain collectives transfer data from their source to
destination buffer.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_coll">MPI Collective Communications</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_csrc"><h3>MPI Collective Communications as Source</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of MPI collective communication operations that are
only sending but not receiving data.  Examples are the non-root
processes in gather and reduction operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_coll">MPI Collective Communications</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_cdst"><h3>MPI Collective Communications as Destination</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of MPI collective communication operations that are
only receiving but not sending data.  Examples are broadcasts
and scatters (for ranks other than the root).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_coll">MPI Collective Communications</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_rma"><h3>MPI One-sided Communication Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms">MPI Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#comms_rma_puts">MPI One-sided Put Communication Operations</a><br/>
    <a href="#comms_rma_gets">MPI One-sided Get Communication Operations</a><br/>
    <a href="#comms_rma_atomics">MPI One-sided Atomic Communication Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_rma_puts"><h3>MPI One-sided Put Communication Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided put communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_rma">MPI One-sided Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_rma_gets"><h3>MPI One-sided Get Communication Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided get communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_rma">MPI One-sided Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="comms_rma_atomics"><h3>MPI One-sided Atomic Communication Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total number of MPI one-sided atomic communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_rma">MPI One-sided Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes"><h3>MPI Bytes Transferred</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
The total number of bytes that were notionally processed in MPI
communication and synchronization operations (i.e., the sum of the bytes
that were sent and received).  Note that the actual number of bytes
transferred is typically not determinable, as this is dependant on the MPI
internal implementation, including message transfer and failed delivery
recovery protocols.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree to break down the bytes transferred into
constituent classes.  Expand the call tree to identify where most data
is transferred and examine the distribution of data transferred by each
process.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#bytes_p2p">MPI Point-to-point Bytes Transferred</a><br/>
    <a href="#bytes_coll">MPI Collective Bytes Transferred</a><br/>
    <a href="#bytes_rma">MPI One-Sided Bytes Transferred</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_p2p"><h3>MPI Point-to-point Bytes Transferred</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
The total number of bytes that were notionally processed by
MPI point-to-point communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to identify where the most data is transferred
using point-to-point communication and examine the distribution of data
transferred by each process.  Compare with the number of <a href="#comms_p2p">MPI Point-to-point Communication Operations</a>
and resulting <a href="#mpi_point2point">MPI Point-to-point Communication Time</a>.
</dd><p><dd>
Average message size can be determined by dividing by the number of MPI
<a href="#comms_p2p">MPI Point-to-point Communication Operations</a> (for all call paths or for particular call paths or
communication operations).  Instead of large numbers of small
communications streamed to the same destination, it may be more
efficient to pack data into fewer larger messages (e.g., using MPI
datatypes).  Very large messages may require a rendezvous between
sender and receiver to ensure sufficient transmission and receipt
capacity before sending commences: try splitting large messages into
smaller ones that can be transferred asynchronously and overlapped with
computation.  (Some MPI implementations allow tuning of the rendezvous
threshold and/or transmission capacity, e.g., via environment
variables.)
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes">MPI Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#bytes_sent">MPI Point-to-point Bytes Sent</a><br/>
    <a href="#bytes_rcvd">MPI Point-to-point Bytes Received</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_sent"><h3>MPI Point-to-point Bytes Sent</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally sent using MPI
point-to-point communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is sent using
point-to-point communication operations and examine the distribution of
data sent by each process.  Compare with the number of <a href="#comms_send">MPI Point-to-point Send Communication Operations</a>
and resulting <a href="#mpi_point2point">MPI Point-to-point Communication Time</a>.
</dd><p><dd>
If the <em>aggregate</em> <a href="#bytes_rcvd">MPI Point-to-point Bytes Received</a> is less than the amount
sent, some messages were cancelled, received into buffers which were
too small, or simply not received at all.  (Generally only aggregate
values can be compared, since sends and receives take place on
different callpaths and on different processes.)  Sending more data than
is received wastes network bandwidth.  Applications do not conform to
the MPI standard when they do not receive all messages that are sent,
and the unreceived messages degrade performance by consuming network
bandwidth and/or occupying message buffers.  Cancelling send operations
is typically expensive, since it usually generates one or more internal
messages.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes_p2p">MPI Point-to-point Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_rcvd"><h3>MPI Point-to-point Bytes Received</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally received using MPI
point-to-point communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is received using
point-to-point communication and examine the distribution of data
received by each process.  Compare with the number of <a href="#comms_recv">MPI Point-to-point Receive Communication Operations</a>
and resulting <a href="#mpi_point2point">MPI Point-to-point Communication Time</a>.
</dd><p><dd>
If the <em>aggregate</em> <a href="#bytes_sent">MPI Point-to-point Bytes Sent</a> is greater than the amount
received, some messages were cancelled, received into buffers which
were too small, or simply not received at all.  (Generally only
aggregate values can be compared, since sends and receives take place
on different callpaths and on different processes.)  Applications do
not conform to the MPI standard when they do not receive all messages
that are sent, and the unreceived messages degrade performance by
consuming network bandwidth and/or occupying message buffers.
Cancelling receive operations may be necessary where speculative
asynchronous receives are employed, however, managing the associated
requests also involves some overhead.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes_p2p">MPI Point-to-point Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_coll"><h3>MPI Collective Bytes Transferred</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
The total number of bytes that were notionally processed in
MPI collective communication operations.  This assumes that collective
communications are implemented naively using point-to-point
communications, e.g., a broadcast being implemented as sends to each
member of the communicator (including the root itself).  Note that
effective MPI implementations use optimized algorithms and/or special
hardware, such that the actual number of bytes transferred may be very
different.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is transferred using
collective communication and examine the distribution of data
transferred by each process.  Compare with the number of
<a href="#comms_coll">MPI Collective Communications</a> and resulting <a href="#mpi_collective">MPI Collective Communication Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes">MPI Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#bytes_cout">MPI Collective Bytes Outgoing</a><br/>
    <a href="#bytes_cin">MPI Collective Bytes Incoming</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_cout"><h3>MPI Collective Bytes Outgoing</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally sent by MPI collective
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is transferred using
collective communication and examine the distribution of data outgoing
from each process.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes_coll">MPI Collective Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_cin"><h3>MPI Collective Bytes Incoming</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally received by MPI collective
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is transferred using
collective communication and examine the distribution of data incoming
to each process.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes_coll">MPI Collective Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_rma"><h3>MPI One-Sided Bytes Transferred</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally processed in MPI one-sided
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is transferred using
one-sided communication and examine the distribution of data transferred
by each process.  Compare with the number of <a href="#comms_rma">MPI One-sided Communication Operations</a> and resulting
<a href="#mpi_rma_communication">MPI One-sided Communication Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes">MPI Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#bytes_put">MPI One-sided Bytes Sent</a><br/>
    <a href="#bytes_get">MPI One-sided Bytes Received</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_put"><h3>MPI One-sided Bytes Sent</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally sent in MPI one-sided
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is transferred using
one-sided communication and examine the distribution of data sent by
each process.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes_rma">MPI One-Sided Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="bytes_get"><h3>MPI One-sided Bytes Received</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The number of bytes that were notionally received in MPI one-sided
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Bytes</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the calltree to see where the most data is transferred using
one-sided communication and examine the distribution of data received by
each process.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#bytes_rma">MPI One-Sided Bytes Transferred</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_slr_count"><h3>MPI Late Receiver Instances (Synchronizations)</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of Late Receiver instances (see
<a href="#mpi_latereceiver">MPI Late Receiver Time</a> for details) found in MPI point-to-point
synchronization operations (i.e., zero-sized message transfers).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs_send">MPI Point-to-point Send Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_sls_count"><h3>MPI Late Sender Instances (Synchronizations)</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of Late Sender instances (see
<a href="#mpi_latesender">MPI Late Sender Time</a> for details) found in MPI point-to-point
synchronization operations (i.e., zero-sized message transfers).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#syncs_recv">MPI Point-to-point Receive Synchronization Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_slswo_count">MPI Late Sender, Wrong Order Instances (Synchronizations)</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_slswo_count"><h3>MPI Late Sender, Wrong Order Instances (Synchronizations)</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of Late Sender instances found in MPI
point-to-point synchronization operations (i.e., zero-sized message
transfers) where messages are received in wrong order (see also
<a href="#mpi_latesender_wo">MPI Late Sender, Wrong Order Time</a>).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_sls_count">MPI Late Sender Instances (Synchronizations)</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_clr_count"><h3>MPI Late Receiver Instances (Communications)</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of Late Receiver instances (see
<a href="#mpi_latereceiver">MPI Late Receiver Time</a> for details) found in MPI point-to-point
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_send">MPI Point-to-point Send Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_cls_count"><h3>MPI Late Sender Instances (Communications)</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of Late Sender instances (see
<a href="#mpi_latesender">MPI Late Sender Time</a> for details) found in MPI point-to-point
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#comms_recv">MPI Point-to-point Receive Communication Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_clswo_count">MPI Late Sender, Wrong Order Instances (Communications)</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_clswo_count"><h3>MPI Late Sender, Wrong Order Instances (Communications)</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Provides the total number of Late Sender instances found in MPI
point-to-point communication operations where messages are received
in wrong order (see also <a href="#mpi_latesender_wo">MPI Late Sender, Wrong Order Time</a>).
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_cls_count">MPI Late Sender Instances (Communications)</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_ops"><h3>MPI File Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of MPI file operations of any type.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree to see the breakdown of different classes of MPI
file operation, expand the calltree to see where they occur, and look
at the distribution of operations done by each process.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_file_iops">MPI Individual File Operations</a><br/>
    <a href="#mpi_file_cops">MPI Collective File Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_iops"><h3>MPI Individual File Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of individual MPI file operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the distribution of individual MPI file operations done by each
process and compare with the corresponding <a href="#mpi_mgmt_file">MPI File Management Time</a> and
<a href="#mpi_io_individual">MPI Individual File I/O Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_file_ops">MPI File Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_file_irops">MPI Individual File Read Operations</a><br/>
    <a href="#mpi_file_iwops">MPI Individual File Write Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_irops"><h3>MPI Individual File Read Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of individual MPI file read operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the callpaths where individual MPI file reads occur and the
distribution of operations done by each process in them, and compare
with the corresponding <a href="#mpi_io_individual">MPI Individual File I/O Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_file_iops">MPI Individual File Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_iwops"><h3>MPI Individual File Write Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of individual MPI file write operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the callpaths where individual MPI file writes occur and the
distribution of operations done by each process in them, and compare
with the corresponding <a href="#mpi_io_individual">MPI Individual File I/O Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_file_iops">MPI Individual File Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_cops"><h3>MPI Collective File Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of collective MPI file operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the distribution of collective MPI file operations done by each
process and compare with the corresponding <a href="#mpi_mgmt_file">MPI File Management Time</a> and
<a href="#mpi_io_collective">MPI Collective File I/O Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_file_ops">MPI File Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_file_crops">MPI Collective File Read Operations</a><br/>
    <a href="#mpi_file_cwops">MPI Collective File Write Operations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_crops"><h3>MPI Collective File Read Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of collective MPI file read operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the callpaths where collective MPI file reads occur and the
distribution of operations done by each process in them, and compare
with the corresponding <a href="#mpi_io_collective">MPI Collective File I/O Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_file_cops">MPI Collective File Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_file_cwops"><h3>MPI Collective File Write Operations</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Number of collective MPI file write operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Examine the callpaths where collective MPI file writes occur and the
distribution of operations done by each process in them, and compare
with the corresponding <a href="#mpi_io_collective">MPI Collective File I/O Time</a>.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_file_cops">MPI Collective File Operations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_wait_at_create"><h3>MPI Wait at Window Create Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in <tt>MPI_Win_create</tt> for the last process to join
in the collective creation of an MPI window handle.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_mgmt_win">MPI Window Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_wait_at_free"><h3>MPI Wait at Window Free Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in <tt>MPI_Win_free</tt> for the last process to join
in the collective deallocation of an MPI window handle.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_mgmt_win">MPI Window Management Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_synchronization"><h3>MPI One-sided Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI one-sided synchronization calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_synchronization">MPI Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_sync_active">MPI Active Target Synchronization Time</a><br/>
    <a href="#mpi_rma_sync_passive">MPI One-sided Passive Target Synchronization Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_sync_active"><h3>MPI Active Target Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI one-sided active target synchronization calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_synchronization">MPI One-sided Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_sync_late_post">MPI Late Post Time in Synchronizations</a><br/>
    <a href="#mpi_rma_early_wait">MPI Early Wait Time</a><br/>
    <a href="#mpi_rma_wait_at_fence">MPI Wait at Fence Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_sync_late_post"><h3>MPI Late Post Time in Synchronizations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI one-sided active target access epoch synchronization
operations, waiting for the corresponding exposure epoch to start.
</dd><p><dd>
<br>
<div align="center">
<img src="RmaLatePost.png" alt="MPI Late Post (Synchronization) Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_sync_active">MPI Active Target Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_early_wait"><h3>MPI Early Wait Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Idle time spent in <tt>MPI_Win_wait</tt>, waiting for the last corresponding
exposure epoch to finish.
</dd><p><dd>
<br>
<div align="center">
<img src="RmaEarlyWait.png" alt="MPI Early Wait Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_sync_active">MPI Active Target Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_late_complete">MPI Late Complete Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_late_complete"><h3>MPI Late Complete Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in the 'Early Wait' inefficiency pattern (see
<a href="#mpi_rma_early_wait">MPI Early Wait Time</a>) due to a late completion of a corresponding
access epoch.  It refers to the timespan between the last RMA access
and the last <tt>MPI_Win_complete</tt> call.
</dd><p><dd>
<br>
<div align="center">
<img src="RmaLateComplete.png" alt="MPI Late Complete Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_early_wait">MPI Early Wait Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_wait_at_fence"><h3>MPI Wait at Fence Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in <tt>MPI_Win_fence</tt> waiting for other participating
processes to reach the fence synchronization.
</dd><p><dd>
<br>
<div align="center">
<img src="RmaWaitAtFence.png" alt="MPI Wait at Fence Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_sync_active">MPI Active Target Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_early_fence">MPI Early Fence Time</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_early_fence"><h3>MPI Early Fence Time</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in <tt>MPI_Win_fence</tt> waiting for outstanding one-sided
communication operations to this location to finish.
</dd><p><dd>
<br>
<div align="center">
<img src="RmaEarlyFence.png" alt="MPI Early Fence Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_wait_at_fence">MPI Wait at Fence Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_sync_passive"><h3>MPI One-sided Passive Target Synchronization Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI one-sided passive target synchronization calls.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_synchronization">MPI One-sided Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_sync_lock_competition">MPI Lock Contention Time in Synchronizations</a><br/>
    <a href="#mpi_rma_sync_wait_for_progress">MPI Wait for Progress Time in Synchronizations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_sync_lock_competition"><h3>MPI Lock Contention Time in Synchronizations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in <tt>MPI_Win_lock</tt> or <tt>MPI_Win_unlock</tt>
before the lock on a window is acquired.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_sync_passive">MPI One-sided Passive Target Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_sync_wait_for_progress"><h3>MPI Wait for Progress Time in Synchronizations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in <tt>MPI_Win_lock</tt> or <tt>MPI_Win_unlock</tt>
until the target is calling into an MPI API function that ensures remote
progress.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_sync_passive">MPI One-sided Passive Target Synchronization Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_communication"><h3>MPI One-sided Communication Time</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI one-sided communication operations, for example,
<tt>MPI_Accumulate</tt>, <tt>MPI_Put</tt>, or <tt>MPI_Get</tt>.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_communication">MPI Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_comm_late_post">MPI Late Post Time in Communications</a><br/>
    <a href="#mpi_rma_comm_lock_competition">MPI Lock Contention Time in Communications</a><br/>
    <a href="#mpi_rma_comm_wait_for_progress">MPI Wait for Progress Time in Communications</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_comm_late_post"><h3>MPI Late Post Time in Communications</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent in MPI one-sided communication operations waiting for the
corresponding exposure epoch to start.
</dd><p><dd>
<br>
<div align="center">
<img src="RmaLatePost.png" alt="MPI Late Post (Communication) Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_communication">MPI One-sided Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_comm_lock_competition"><h3>MPI Lock Contention Time in Communications</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in <tt>MPI_Win_lock</tt> or <tt>MPI_Win_unlock</tt>
before the lock on a window is acquired.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_communication">MPI One-sided Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_comm_wait_for_progress"><h3>MPI Wait for Progress Time in Communications</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Time spent waiting in <tt>MPI_Win_lock</tt> or <tt>MPI_Win_unlock</tt>
until the target is calling into an MPI API function that ensures remote
progress.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_communication">MPI One-sided Communication Time</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_pairsync_count"><h3>Pair-wise MPI One-sided Synchronizations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
MPI one-sided synchronization methods may synchronize processes when they
need to ensure that no further one-sided communication operation will take
place in the epoch to be closed.  The MPI pair-wise one-sided
synchronization metric counts the number of remote processes it potentially
has to wait for at the end of this epoch, e.g., at a fence a process will
wait for every other process with the same window handle in this
barrier-like construct.  This is required, as the target has no knowledge
of whether a certain remote process has already completed its access epoch.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A large count of pair-wise synchronizations indicate a tight coupling of
the processes.  A developer should then check, whether the level of
inter-process coupling is needed for her algorithm, or whether an algorithm
with looser coupling may be beneficial.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_rma_pairsync_unneeded_count">Unneeded Pair-wise MPI One-sided Synchronizations</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_rma_pairsync_unneeded_count"><h3>Unneeded Pair-wise MPI One-sided Synchronizations</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
The unneeded pair-wise MPI one-sided synchronizations express the number of
situations where a process synchronized with a remote process at the end
of an epoch, although no one-sided operation from that remote process has
taken place in the corresponding epoch.  A synchronization therefore would
not be necessary to ensure consistency, and may decrease performance
through over-synchronization.
</dd>
<dt><b>Unit:</b></dt>
<dd>Counts</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A high number of unneeded synchronizations indicates that a different
synchronization mechanism, i.e., choosing general active target
synchronization (GATS) over fence, or more refined/precise access groups
within GATS may be beneficial to the application's performance.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_rma_pairsync_count">Pair-wise MPI One-sided Synchronizations</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="imbalance"><h3>Computational Load Imbalance Heuristic</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This simple heuristic allows to identify computational load imbalances and
is calculated for each (call-path, process/thread) pair.  Its value
represents the absolute difference to the average computation time.  This
average value is the aggregated exclusive time spent by all
processes/threads in this call-path, divided by the number of
processes/threads visiting it.
</dd><p><dd>
<br>
<div align="center">
<img src="Imbalance.png" alt="Computational load imbalance Example">
</div>
<br>

</dd><p><dd>
<b>Note:</b>
A high value for a collapsed call tree node does not necessarily mean that
there is a load imbalance in this particular node, but the imbalance can
also be somewhere in the subtree underneath.  Unused threads outside
of OpenMP parallel regions are considered to constitute <a href="#omp_idle_threads">OpenMP Idle Threads Time</a>
and expressly excluded from the computational load imbalance heuristic.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Total load imbalance comprises both above average computation time
and below average computation time, therefore at most half of it could
potentially be recovered with perfect (zero-overhead) load balance
that distributed the excess from overloaded to unloaded
processes/threads, such that all took exactly the same time.
</dd><p><dd>
Computation imbalance is often the origin of communication and
synchronization inefficiencies, where processes/threads block and
must wait idle for partners, however, work partitioning and
parallelization overheads may be prohibitive for complex computations
or unproductive for short computations.  Replicating computation on
all processes/threads will eliminate imbalance, but would typically
not result in recover of this imbalance time (though it may reduce
associated communication and synchronization requirements).
</dd><p><dd>
Call paths with significant amounts of computational imbalance should
be examined, along with processes/threads with above/below-average
computation time, to identify parallelization inefficiencies.  Call paths
executed by a subset of processes/threads may relate to parallelization
that hasn't been fully realized (<a href="#imbalance_below_bypass">Computational Load Imbalance Heuristic: Non-participation</a>), whereas
call-paths executed only by a single process/thread
(<a href="#imbalance_above_single">Computational Load Imbalance Heuristic: Single Participant</a>) often represent unparallelized serial code,
which will be scalability impediments as the number of processes/threads
increase.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#imbalance_above">Computational Load Imbalance Heuristic: Overload</a><br/>
    <a href="#imbalance_below">Computational Load Imbalance Heuristic: Underload</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="imbalance_above"><h3>Computational Load Imbalance Heuristic: Overload</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This metric identifies processes/threads where the exclusive execution
time spent for a particular call-path was above the average value.
It is a complement to <a href="#imbalance_below">Computational Load Imbalance Heuristic: Underload</a>.
</dd><p><dd>
<br>
<div align="center">
<img src="Imbal_Overload.png" alt="Overload Example">
</div>
<br>

</dd><p><dd>
See <a href="#imbalance">Computational Load Imbalance Heuristic</a> for details on how this heuristic is calculated.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
The CPU time which is above the average time for computation is the
maximum that could potentially be recovered with perfect (zero-overhead)
load balance that distributed the excess from overloaded to underloaded
processes/threads.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#imbalance">Computational Load Imbalance Heuristic</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#imbalance_above_single">Computational Load Imbalance Heuristic: Single Participant</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="imbalance_above_single"><h3>Computational Load Imbalance Heuristic: Single Participant</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This heuristic distinguishes the execution time for call-paths executed
by single processes/threads that potentially could be recovered with
perfect parallelization using all available processes/threads.
</dd><p><dd>
It is the <a href="#imbalance_above">Computational Load Imbalance Heuristic: Overload</a> time for call-paths that only have
non-zero <a href="#visits">Visits</a> for one process or thread, and complements
<a href="#imbalance_below_singularity">Computational Load Imbalance Heuristic: Non-participation in Singularity</a>.
</dd><p><dd>
<br>
<div align="center">
<img src="Imbal_Single.png" alt="Single participant Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
This time is often associated with activities done exclusively by a
"Master" process/thread (often rank 0) such as initialization,
finalization or I/O, but can apply to any process/thread that
performs computation that none of its peers do (or that does its
computation on a call-path that differs from the others).
</dd><p><dd>
The CPU time for singular execution of the particular call path
typically presents a serial bottleneck impeding scalability as none of
the other available processes/threads are being used, and
they may well wait idling until the result of this computation becomes
available.  (Check the MPI communication and synchronization times,
particularly waiting times, for proximate call paths.)
In such cases, even small amounts of singular execution can
have substantial impact on overall performance and parallel efficiency.
With perfect partitioning and (zero-overhead) parallel
execution of the computation, it would be possible to recover this time.
</dd><p><dd>
When the amount of time is small compared to the total execution time,
or when the cost of parallelization is prohibitive, it may not be
worth trying to eliminate this inefficiency.  As the number of
processes/threads are increased and/or total execution time decreases,
however, the relative impact of this inefficiency can be expected to grow.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#imbalance_above">Computational Load Imbalance Heuristic: Overload</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="imbalance_below"><h3>Computational Load Imbalance Heuristic: Underload</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This metric identifies processes/threads where the computation time spent
for a particular call-path was below the average value.  It is a complement
to <a href="#imbalance_above">Computational Load Imbalance Heuristic: Overload</a>.
</dd><p><dd>
<br>
<div align="center">
<img src="Imbal_Underload.png" alt="Underload Example">
</div>
<br>

</dd><p><dd>
See <a href="#imbalance">Computational Load Imbalance Heuristic</a> for details on how this heuristic is calculated.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
The CPU time which is below the average time for computation could
potentially be used to reduce the excess from overloaded processes/threads
with perfect (zero-overhead) load balancing.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#imbalance">Computational Load Imbalance Heuristic</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#imbalance_below_bypass">Computational Load Imbalance Heuristic: Non-participation</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="imbalance_below_bypass"><h3>Computational Load Imbalance Heuristic: Non-participation</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This heuristic distinguishes the execution time for call paths not executed
by a subset of processes/threads that potentially could be used with
perfect parallelization using all available processes/threads.
</dd><p><dd>
It is the <a href="#imbalance_below">Computational Load Imbalance Heuristic: Underload</a> time for call paths which have zero
<a href="#visits">Visits</a> and were therefore not executed by this process/thread.
</dd><p><dd>
<br>
<div align="center">
<img src="Imbal_Bypass.png" alt="Non-participation Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
The CPU time used for call paths where not all processes or threads
are exploited typically presents an ineffective parallelization that
limits scalability, if the unused processes/threads wait idling for
the result of this computation to become available.  With perfect
partitioning and (zero-overhead) parallel execution of the computation,
it would be possible to recover this time.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#imbalance_below">Computational Load Imbalance Heuristic: Underload</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#imbalance_below_singularity">Computational Load Imbalance Heuristic: Non-participation in Singularity</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="imbalance_below_singularity"><h3>Computational Load Imbalance Heuristic: Non-participation in Singularity</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This heuristic distinguishes the execution time for call paths not executed
by all but a single process/thread that potentially could be recovered with
perfect parallelization using all available processes/threads.
</dd><p><dd>
It is the <a href="#imbalance_below">Computational Load Imbalance Heuristic: Underload</a> time for call paths that only have
non-zero <a href="#visits">Visits</a> for one process/thread, and complements
<a href="#imbalance_above_single">Computational Load Imbalance Heuristic: Single Participant</a>.
</dd><p><dd>
<br>
<div align="center">
<img src="Imbal_Singularity.png" alt="Singularity Example">
</div>
<br>

</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
The CPU time for singular execution of the particular call path
typically presents a serial bottleneck impeding scalability as none of
the other processes/threads that are available are being used, and
they may well wait idling until the result of this computation becomes
available.  With perfect partitioning and (zero-overhead) parallel
execution of the computation, it would be possible to recover this time.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#imbalance_below_bypass">Computational Load Imbalance Heuristic: Non-participation</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="critical_path"><h3>Critical Path Profile</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
This metric provides a profile of the application's critical path.
Following the causality chain from the last active program
process/thread back to the program start, the critical path shows
the call paths and processes/threads that are responsible for the
program's wall-clock runtime.
</dd><p><dd>
<br>
<div align="center">
<img src="CriticalPath.png" alt="Critical path profile Example">
</div>
<br>

</dd><p><dd>
Note that Scalasca does not yet consider POSIX threads when determining
the critical path.  Thus, the critical-path profile is currently incorrect
if POSIX threads are being used, as only the master thread of each process
is taken into account.  However, it may still provide useful insights
across processes for hybrid MPI+Pthreads applications.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Call paths that occupy a lot of time on the critical path are good
optimization candidates.  In contrast, optimizing call paths that do
not appear on the critical path will not improve program runtime.
</dd><p><dd>
Call paths that spend a disproportionately large amount of
time on the critical path with respect to their total execution
time indicate parallel bottlenecks, such as load imbalance or
serial execution.  Use the percentage view modes and compare execution
time and critical path profiles to identify such call paths.
</dd><p><dd>
The system tree pane shows the contribution of individual
processes/threads to the critical path.
However, note that the critical path runs only on one process at a time.
In a well-balanced program, the critical path follows a
more-or-less random course across processes and may not visit many
processes at all.
Therefore, a high critical-path time on individual processes does not
necessarily indicate a performance problem.
Exceptions are significant load imbalances or serial execution on
single processes.
Use the critical-path imbalance metric or compare with the distribution
of execution time across processes to identify such cases.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#critical_path_imbalance">Critical-Path Imbalance</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="critical_path_imbalance"><h3>Critical-Path Imbalance</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This metric highlights parallel performance bottlenecks.
</dd><p><dd>
In essence, the critical-path imbalance is the positive difference of
the time a call path occupies on the critical path and the call path's
average runtime across all CPU locations.  Thus, a high critical-path
imbalance identifies call paths which spend a disproportionate amount
of time on the critical path.
</dd><p><dd>
<br>
<div align="center">
<img src="CriticalPathImbalance.png" alt="Imbalance Example">
</div>
<br>

</dd><p><dd>
The image above illustrates the critical-path profile and the critical-path
imbalance for the example in the <a href="#critical_path">Critical Path Profile</a> metric description.
Note that the excess time of regions <tt>foo</tt> and <tt>baz</tt> on the
critical path compared to their average execution time is marked as
imbalance.  While also on the critical path, region <tt>bar</tt> is
perfectly balanced between the processes and therefore has no contribution
to critical-path imbalance.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A high critical-path imbalance indicates a parallel bottleneck,
such as load imbalance or serial execution.  Cross-analyze with
other metrics, such as the distribution of execution time across
CPU locations, to identify the type and causes of the parallel
bottleneck.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#critical_path">Critical Path Profile</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="performance_impact"><h3>Performance Impact</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This heuristic characterizes the performance impact of program activities
(call paths) on the program as a whole.  This includes the activities'
direct impact on the CPU time, as well as their indirect impact through
load imbalance.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree hierarchy to identify the impact of activities on
the critical path of the application compared to activities not located
on the critical path.  For critical-path activities, further expand the
<a href="#performance_impact_criticalpath">Critical-path Activities</a> hierarchy to identify how much of
the performance impact is due to imbalance rather than actual computation.
</dd><p><dd>
Expand the call tree to identify important callpaths and routines with the
most impact on overall resource consumption.
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#performance_impact_criticalpath">Critical-path Activities</a><br/>
    <a href="#non_critical_path_activities">Non-critical-path Activities</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="performance_impact_criticalpath"><h3>Critical-path Activities</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Overall resource comsumption caused by activities that appear on the
critical path. While the <a href="#critical_path">Critical Path Profile</a> metric calculates a profile
of the critical path and thus also highlights the processes/threads taking
part in its execution, this metric aggregates the overall resource
consumption associated with the execution of critical-path activities,
including any waiting times on processes/threads not on the critical path.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Expand the metric tree hierarchy to break down the overall resource
consumption into the fraction that is caused by executing the critical-path
activities themselves and the resources consumed by wait states caused by
imbalances in these activities.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#performance_impact">Performance Impact</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#critical_path_activities">Activity Impact</a><br/>
    <a href="#critical_imbalance_impact">Critical Imbalance Impact</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="critical_path_activities"><h3>Activity Impact</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Resource consumption caused by executing activities that appear on the
critical path.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#performance_impact_criticalpath">Critical-path Activities</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="critical_imbalance_impact"><h3>Critical Imbalance Impact</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
This heuristic maps waiting time onto activities that spend &quot;too
much&quot; time on the critical path, highlighting imbalanced activities
that are likely responsible for wait states.
</dd><p><dd>
Unlike the <a href="#delay">Delay Costs</a> metric which identifies any delay which leads to a
wait state at a synchronization point, the imbalance impact pinpoints
inefficiencies which have a global runtime effect by mapping overall
resource consumption to call paths that appear on the critical path.  This
allows to distinguish different types of imbalances, for example,
<a href="#intra_partition_imbalance">Intra-partition Imbalance</a> and <a href="#inter_partition_imbalance">Inter-partition Imbalance</a>, which
are especially useful for the analysis of MPMD applications.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A high imbalance impact indicates a parallel bottleneck.
Expand the metric tree hierarchy to distinguish between intra- and
inter-partition imbalances.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#performance_impact_criticalpath">Critical-path Activities</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#intra_partition_imbalance">Intra-partition Imbalance</a><br/>
    <a href="#inter_partition_imbalance">Inter-partition Imbalance</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="intra_partition_imbalance"><h3>Intra-partition Imbalance</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Resource consumption caused by imbalances within process partitions that
perform activities on the critical path.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A high intra-partition imbalance impact indicates that imbalances
<i>within</i> the dominating (MPMD) partitions cause significant wait
states.  Compare with the <a href="#critical_path_imbalance">Critical-Path Imbalance</a> and <a href="#delay">Delay Costs</a>
metrics to identify the imbalanced processes/threads.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#critical_imbalance_impact">Critical Imbalance Impact</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="inter_partition_imbalance"><h3>Inter-partition Imbalance</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Resource consumption caused by imbalances within process partitions that
perform activities on the critical path.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
A high inter-partition imbalance impact indicates a sub-optimal
partitioning in MPMD applications.  Compare with the <a href="#critical_path">Critical Path Profile</a>
to identify the delaying partition and adjust the process or workload
partitioning accordingly to achieve a better load balance.
</dd><p><dd>
Note that in hybrid MPI+OpenMP SPMD applications, master and worker threads
are also considered as different partitions.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#critical_imbalance_impact">Critical Imbalance Impact</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="non_critical_path_activities"><h3>Non-critical-path Activities</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Overall resource comsumption caused by activities that do not appear on the
critical path.  As such, optimizing these activities does not improve the
application runtime.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#performance_impact">Performance Impact</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay"><h3>Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
This metric highlights the root causes of wait states.  Root causes of
wait states are regions of excess execution time &mdash; delays &mdash;
that cause wait states at subsequent synchronization points.  Whereas
wait states represent the time spent idling at a synchronization point
while waiting for the communication partner(s) to enter the communication
operation, delays show which call paths caused the latecomer at a
synchronization point to be late.  The delay <i>costs</i> indicate the
total amount of waiting time caused by a delay, including indirect
effects of wait-states spreading along the communication chain.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
Call paths and process/threads with high delay costs pinpoint the
location of delays.  In general, shift work/communication load from
processes/threads with high delay costs to processes/threads with large
waiting times.
<p>
Delays fall into three main categories:
<ol>
    <li>
        Computational imbalance
        <br>
        Delays within computational regions, in call paths that are also
        present on processes/threads that exhibit wait states, indicates
        a computational imbalance.  Improve the work load balance by
        shifting workload within these call paths from processes/threads
        that are delayed to processes/threads that are waiting.
    </li>
    <li>
        Communication imbalance
        <br>
        Delay costs within communication functions indicate an imbalanced
        communication load or inefficient communication pattern.
    </li>
    <li>
        Inefficient parallelism
        <br>
        Delays in call paths that are only present on a single or a small
        subset of processes/threads indicates inefficient parallelism.
        Reduce the time spent in such functions.
    </li>
</ol>
</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_mpi">MPI Delay Costs</a><br/>
    <a href="#delay_omp">OpenMP Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_mpi"><h3>MPI Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total costs and locations of delays that cause wait states in MPI
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay">Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_p2p">MPI Point-to-point Delay Costs</a><br/>
    <a href="#delay_collective">MPI Collective Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_p2p"><h3>MPI Point-to-point Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays that cause wait states in MPI point-to-point
communication.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_mpi">MPI Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_latesender_aggregate">MPI Late Sender Delay Costs</a><br/>
    <a href="#delay_latereceiver_aggregate">MPI Late Receiver Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_latesender_aggregate"><h3>MPI Late Sender Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations  of delays that cause Late Sender wait states in MPI
point-to-point communication.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_p2p">MPI Point-to-point Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_latesender">Short-term MPI Late Sender Delay Costs</a><br/>
    <a href="#delay_latesender_longterm">Long-term MPI Late Sender Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_latesender"><h3>Short-term MPI Late Sender Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term delay costs reflect the direct effect of load or communication
imbalance on MPI Late Sender wait states.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Because of this overload, the affected processes/threads arrive late
at subsequent MPI send operations, thus causing Late Sender wait
states on the remote processes.
</p><br><p>
Compare with <a href="#mpi_latesender">MPI Late Sender Time</a> to identify an imbalance pattern.
Try to reduce workload in the affected call paths.
Alternatively, shift workload in the affected call paths from
processes/threads with delay costs to processes/threads that
exhibit late-sender wait states.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_latesender_aggregate">MPI Late Sender Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_latesender_longterm"><h3>Long-term MPI Late Sender Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.  That is, the wait states caused by the original
computational overload spread along the communication chain to
remote locations.
</p><br><p>
Try to reduce workload in the affected call paths, or shift workload from
processes/threads with delay costs to processes/threads that exhibit
Late Sender wait states.
Try to implement a more asynchronous communication pattern that can
compensate for small imbalances, e.g., by using non-blocking instead
of blocking communication.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_latesender_aggregate">MPI Late Sender Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_latereceiver_aggregate"><h3>MPI Late Receiver Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays that cause Late Receiver wait states in MPI
point-to-point communication.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_p2p">MPI Point-to-point Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_latereceiver">Short-term MPI Late Receiver Delay Costs</a><br/>
    <a href="#delay_latereceiver_longterm">Long-term MPI Late Receiver Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_latereceiver"><h3>Short-term MPI Late Receiver Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term delay costs reflect the direct effect of load or communication
imbalance on MPI Late Receiver wait states.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Because of this overload, the affected processes/threads arrive late
at subsequent MPI receive operations, thus causing Late Receiver wait
states on the remote processes.
</p><br><p>
Compare with <a href="#mpi_latereceiver">MPI Late Receiver Time</a> to identify an imbalance pattern.
Try to reduce workload in the affected call paths.
Alternatively, shift workload in the affected call paths from
processes/threads with delay costs to processes/threads that
exhibit late-receiver wait states.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_latereceiver_aggregate">MPI Late Receiver Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_latereceiver_longterm"><h3>Long-term MPI Late Receiver Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.  That is, the wait states caused by the original
computational overload spread along the communication chain to
remote locations.
</p><br><p>
Try to reduce workload in the affected call paths, or shift workload from
processes/threads with delay costs to processes/threads that exhibit
Late Receiver wait states.
Try to implement a more asynchronous communication pattern that can
compensate for small imbalances, e.g. by using non-blocking instead
of blocking communication.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_latereceiver_aggregate">MPI Late Receiver Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_collective"><h3>MPI Collective Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays causing wait states in MPI collective
communication.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_mpi">MPI Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_barrier_aggregate">MPI Wait at Barrier Delay Costs</a><br/>
    <a href="#delay_n2n_aggregate">MPI Wait at N x N Delay Costs</a><br/>
    <a href="#delay_12n_aggregate">MPI Late Broadcast Delay Costs</a><br/>
    <a href="#delay_n21_aggregate">MPI Early Reduce Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_barrier_aggregate"><h3>MPI Wait at Barrier Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays that cause wait states in MPI barrier
synchronizations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_collective">MPI Collective Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_barrier">Short-term MPI Barrier Delay Costs</a><br/>
    <a href="#delay_barrier_longterm">Long-term MPI Barrier Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_barrier"><h3>Short-term MPI Barrier Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term delay costs reflect the direct effect of load or communication
imbalance on MPI barrier wait states.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Refer to <a href="#delay_latesender">Short-term MPI Late Sender Delay Costs</a> for more information on reducing delay
costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_barrier_aggregate">MPI Wait at Barrier Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_barrier_longterm"><h3>Long-term MPI Barrier Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.
Refer to <a href="#delay_latesender_longterm">Long-term MPI Late Sender Delay Costs</a> for more information on reducing
long-term delay costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_barrier_aggregate">MPI Wait at Barrier Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_n2n_aggregate"><h3>MPI Wait at N x N Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays that cause wait states in MPI n-to-n
collective communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_collective">MPI Collective Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_n2n">Short-term MPI N x N Collectives Delay Costs</a><br/>
    <a href="#delay_n2n_longterm">Long-term MPI N x N Collectives Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_n2n"><h3>Short-term MPI N x N Collectives Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term costs reflect the direct effect of load or communication
imbalance on wait states in MPI n-to-n collective communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Refer <a href="#delay_latesender">Short-term MPI Late Sender Delay Costs</a> for more information on reducing delay
costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_n2n_aggregate">MPI Wait at N x N Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_n2n_longterm"><h3>Long-term MPI N x N Collectives Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.
Refer to <a href="#delay_latesender_longterm">Long-term MPI Late Sender Delay Costs</a> for more information on reducing
long-term delay costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_n2n_aggregate">MPI Wait at N x N Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_12n_aggregate"><h3>MPI Late Broadcast Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs of delays that cause wait states in collective MPI 1-to-n
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_collective">MPI Collective Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_12n">Short-term MPI 1-to-N Collectives Delay Costs</a><br/>
    <a href="#delay_12n_longterm">Long-term MPI 1-to-N Collectives Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_12n"><h3>Short-term MPI 1-to-N Collectives Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term costs reflect the direct effect of load or communication
imbalance on wait states in on MPI 1-to-n collectives.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Refer <a href="#delay_latesender">Short-term MPI Late Sender Delay Costs</a> for more information on reducing delay
costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_12n_aggregate">MPI Late Broadcast Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_12n_longterm"><h3>Long-term MPI 1-to-N Collectives Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.
Refer to <a href="#delay_latesender_longterm">Long-term MPI Late Sender Delay Costs</a> for more information on reducing
long-term delay costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_12n_aggregate">MPI Late Broadcast Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_n21_aggregate"><h3>MPI Early Reduce Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs of delays that cause wait states in collective MPI n-to-1
communication operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_collective">MPI Collective Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_n21">Short-term MPI N-to-1 Collectives Delay Costs</a><br/>
    <a href="#delay_n21_longterm">Long-term MPI N-to-1 Collectives Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_n21"><h3>Short-term MPI N-to-1 Collectives Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term costs reflect the direct effect of load or communication
imbalance on wait states in on MPI n-to-1 collectives.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Refer <a href="#delay_latesender">Short-term MPI Late Sender Delay Costs</a> for more information on reducing delay
costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_n21_aggregate">MPI Early Reduce Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_n21_longterm"><h3>Long-term MPI N-to-1 Collectives Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.
Refer to <a href="#delay_latesender_longterm">Long-term MPI Late Sender Delay Costs</a> for more information on reducing
long-term delay costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_n21_aggregate">MPI Early Reduce Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_omp"><h3>OpenMP Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Total costs and locations of delays that cause wait states in OpenMP
constructs.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay">Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_ompbarrier_aggregate">OpenMP Wait at Barrier Delay Costs</a><br/>
    <a href="#delay_ompidle_aggregate">OpenMP Thread Idleness Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_ompbarrier_aggregate"><h3>OpenMP Wait at Barrier Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays that cause wait states in OpenMP barrier
synchronizations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_omp">OpenMP Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_ompbarrier">Short-term OpenMP Barrier Delay Costs</a><br/>
    <a href="#delay_ompbarrier_longterm">Long-term OpenMP Barrier Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_ompbarrier"><h3>Short-term OpenMP Barrier Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term costs reflect the direct effect of load or communication
imbalance on OpenMP barrier wait states.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs indicate a computation or communication
overload in/on the affected call paths and processes/threads.
Refer to <a href="#delay_latesender">Short-term MPI Late Sender Delay Costs</a> for more information on reducing delay
costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_ompbarrier_aggregate">OpenMP Wait at Barrier Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_ompbarrier_longterm"><h3>Long-term OpenMP Barrier Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects.
Refer to <a href="#delay_latesender_longterm">Long-term MPI Late Sender Delay Costs</a> for more information on reducing
long-term delay costs in general.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_ompbarrier_aggregate">OpenMP Wait at Barrier Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_ompidle_aggregate"><h3>OpenMP Thread Idleness Delay Costs</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Costs and locations of delays that cause OpenMP worker threads to idle.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
See <a href="#delay">Delay Costs</a> for details.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_omp">OpenMP Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#delay_ompidle">Short-term OpenMP Thread Idleness Delay Costs</a><br/>
    <a href="#delay_ompidle_longterm">Long-term OpenMP Thread Idleness Delay Costs</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_ompidle"><h3>Short-term OpenMP Thread Idleness Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Short-term costs reflect the direct effect of sections outside of OpenMP
parallel regions on thread idleness.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High short-term delay costs for thread idleness indicates that much time
is spent outside of OpenMP parallel regions in the affected call paths.
</p><br><p>
Try to reduce workload in the affected call paths.
Alternatively, apply OpenMP parallelism to more sections of the code.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_ompidle_aggregate">OpenMP Thread Idleness Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="delay_ompidle_longterm"><h3>Long-term OpenMP Thread Idleness Delay Costs</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Long-term delay costs reflect indirect effects of load or communication
imbalance on wait states.  That is, they cover waiting time that was caused
indirectly by wait states which themselves delay subsequent communication
operations.
Here, they identify costs and locations of delays that indirectly leave
OpenMP worker threads idle due to wait-state propagation.
In particular, long-term idle thread delay costs indicate call paths
and processes/threads that increase the time worker threads are idling
because of MPI wait states outside of OpenMP parallel regions.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Diagnosis:</b></dt>
<dd>
High long-term delay costs indicate that computation or communication
overload in/on the affected call paths and processes/threads has
far-reaching effects. That is, the wait states caused by the original
computational overload spread along the communication chain to
remote locations.
</p><br><p>
Try to reduce workload in the affected call paths, or shift workload
from processes/threads with delay costs to processes/threads that
exhibit wait states.
Try to implement a more asynchronous communication pattern that can
compensate for small imbalances, e.g., by using non-blocking instead
of blocking communication.
</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#delay_ompidle_aggregate">OpenMP Thread Idleness Delay Costs</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="waitstates_direct_vs_indirect"><h3>MPI Point-to-point Wait State Classification: Direct vs. Indirect</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Partitions MPI point-to-point wait states into waiting time directly caused
by delays and waiting time caused by propagation.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_wait_direct">Direct MPI Point-to-point Wait States</a><br/>
    <a href="#mpi_wait_indirect">Indirect MPI Point-to-point Wait States</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_direct"><h3>Direct MPI Point-to-point Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI point-to-point operations that results from direct
delay, i.e., is directly caused by a load- or communication imbalance.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#waitstates_direct_vs_indirect">MPI Point-to-point Wait State Classification: Direct vs. Indirect</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_wait_direct_latesender">Direct MPI Late Sender Wait States</a><br/>
    <a href="#mpi_wait_direct_latereceiver">Direct MPI Late Receiver Wait States</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_direct_latesender"><h3>Direct MPI Late Sender Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Sender wait states that results from direct delay,
i.e., is caused by load imbalance.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_direct">Direct MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_direct_latereceiver"><h3>Direct MPI Late Receiver Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Receiver wait states that results from direct
delay, i.e., is caused by load imbalance.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_direct">Direct MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_indirect"><h3>Indirect MPI Point-to-point Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI point-to-point operations that results from indirect
delay, i.e., is caused indirectly by wait-state propagation.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#waitstates_direct_vs_indirect">MPI Point-to-point Wait State Classification: Direct vs. Indirect</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_wait_indirect_latesender">Indirect MPI Late Sender Wait States</a><br/>
    <a href="#mpi_wait_indirect_latereceiver">Indirect MPI Late Receiver Wait States</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_indirect_latesender"><h3>Indirect MPI Late Sender Wait States</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Sender wait states that results from indirect
delay, i.e., is caused indirectly by wait-state propagation.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_indirect">Indirect MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_indirect_latereceiver"><h3>Indirect MPI Late Receiver Wait States</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Receiver wait states that results from indirect
delay, i.e., is caused by wait-state propagation.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_indirect">Indirect MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="waitstates_propagating_vs_terminal"><h3>MPI Point-to-point Wait State Classification: Propagating vs. Terminal</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Partitions MPI point-to-point waiting time into wait states that propagate
further (i.e., cause wait states on other processes) and those that do not.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd>None</dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_wait_propagating">Propagating MPI Point-to-point Wait States</a><br/>
    <a href="#mpi_wait_terminal">Terminal MPI Point-to-point Wait States</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_propagating"><h3>Propagating MPI Point-to-point Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI point-to-point operations that propagates further and
causes additional waiting time on other processes.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#waitstates_propagating_vs_terminal">MPI Point-to-point Wait State Classification: Propagating vs. Terminal</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_wait_propagating_ls">Propagating MPI Late Sender Wait States</a><br/>
    <a href="#mpi_wait_propagating_lr">Propagating MPI Late Receiver Wait States</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_propagating_ls"><h3>Propagating MPI Late Sender Wait States</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Sender wait states that propagates further and
causes additional waiting time on other processes.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_propagating">Propagating MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_propagating_lr"><h3>Propagating MPI Late Receiver Wait States</h3></a>
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Receiver wait states that propagates further and
causes additional waiting time on other processes.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_propagating">Propagating MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_terminal"><h3>Terminal MPI Point-to-point Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI point-to-point operations that does not propagate
further.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#waitstates_propagating_vs_terminal">MPI Point-to-point Wait State Classification: Propagating vs. Terminal</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    <a href="#mpi_wait_terminal_ls">Terminal MPI Late Sender Wait States</a><br/>
    <a href="#mpi_wait_terminal_lr">Terminal MPI Late Receiver Wait States</a>
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_terminal_ls"><h3>Terminal MPI Late Sender Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Sender wait states that does not propagate
further.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_terminal">Terminal MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>

<hr width="75%" align="center">

<a name="mpi_wait_terminal_lr"><h3>Terminal MPI Late Receiver Wait States</h3></a>
(only available after <a href="#remapping_info">remapping</a>)
<dl>
<dt><b>Description:</b></dt>
<dd>
Waiting time in MPI Late Receiver wait states that does not propagate
further.
</dd>
<dt><b>Unit:</b></dt>
<dd>Seconds</dd>
<dt><b>Parent metric:</b></dt>
<dd><a href="#mpi_wait_terminal">Terminal MPI Point-to-point Wait States</a></dd>
<dt><b>Sub-metrics:</b></dt>
<dd>
    None
</dd>
</dl>
